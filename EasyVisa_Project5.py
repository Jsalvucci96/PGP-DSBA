# -*- coding: utf-8 -*-
"""DSBA_Project_ET_EasyVisa_Fullcode.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13z6nSTYVR3_QlKiIBowIoVyBsTHKoNi7

# EasyVisa Project

## Context:

Business communities in the United States are facing high demand for human resources, but one of the constant challenges is identifying and attracting the right talent, which is perhaps the most important element in remaining competitive. Companies in the United States look for hard-working, talented, and qualified individuals both locally as well as abroad.

The Immigration and Nationality Act (INA) of the US permits foreign workers to come to the United States to work on either a temporary or permanent basis. The act also protects US workers against adverse impacts on their wages or working conditions by ensuring US employers' compliance with statutory requirements when they hire foreign workers to fill workforce shortages. The immigration programs are administered by the Office of Foreign Labor Certification (OFLC).

OFLC processes job certification applications for employers seeking to bring foreign workers into the United States and grants certifications in those cases where employers can demonstrate that there are not sufficient US workers available to perform the work at wages that meet or exceed the wage paid for the occupation in the area of intended employment.

## Objective:

In FY 2016, the OFLC processed 775,979 employer applications for 1,699,957 positions for temporary and permanent labor certifications. This was a nine percent increase in the overall number of processed applications from the previous year. The process of reviewing every case is becoming a tedious task as the number of applicants is increasing every year.

The increasing number of applicants every year calls for a Machine Learning based solution that can help in shortlisting the candidates having higher chances of VISA approval. OFLC has hired your firm EasyVisa for data-driven solutions. You as a data scientist have to analyze the data provided and, with the help of a classification model:

* Facilitate the process of visa approvals.
* Recommend a suitable profile for the applicants for whom the visa should be certified or denied based on the drivers that significantly influence the case status.


## Data Description

The data contains the different attributes of the employee and the employer. The detailed data dictionary is given below.

* case_id: ID of each visa application
* continent: Information of continent the employee
* education_of_employee: Information of education of the employee
* has_job_experience: Does the employee has any job experience? Y= Yes; N = No
* requires_job_training: Does the employee require any job training? Y = Yes; N = No
* no_of_employees: Number of employees in the employer's company
* yr_of_estab: Year in which the employer's company was established
* region_of_employment: Information of foreign worker's intended region of employment in the US.
* prevailing_wage:  Average wage paid to similarly employed workers in a specific occupation in the area of intended employment. The purpose of the prevailing wage is to ensure that the foreign worker is not underpaid compared to other workers offering the same or similar service in the same area of employment.
* unit_of_wage: Unit of prevailing wage. Values include Hourly, Weekly, Monthly, and Yearly.
* full_time_position: Is the position of work full-time? Y = Full Time Position; N = Part Time Position
* case_status:  Flag indicating if the Visa was certified or denied

## Importing necessary libraries and data
"""

# Commented out IPython magic to ensure Python compatibility.
#library to suppress warnings
import warnings
warnings.filterwarnings("ignore")

#libraries to help read and manipulate data
import numpy as np
import pandas as pd

#libraries to help with data visualization
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

#libraries to split data and impute missing values
!pip install -U scikit-learn --user
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

#libraries to import different classifiers
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier
from sklearn.tree import DecisionTreeClassifier
!pip install xgboost
from xgboost import XGBClassifier

#libtune to tune model and get different metric scores
from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import GridSearchCV

#mounting google drive and loading the dataset
from google.colab import drive
drive.mount('/content/drive')
data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/pgp_module5/EasyVisa.csv')

#creating a copy of the dataset to avoid any changes to the original
visa = data.copy()

"""## Data Overview

- Observations
- Sanity checks
"""

#checking the first and last 5 rows of the dataset
visa.head()

visa.tail()

"""The dataset has been loaded properly."""

#checking the shape of the dataset
visa.shape

"""The dataset has 25,480 rows and 12 columns."""

#checking the data types of the columns
visa.info()

"""All of the columns are of object datatype with the exception of "no_of_employees", "yr_of_estab", and "prevailing_wage", which are continuous datatypes. Our dependent vaiable, "case_status", is an object datatype."""

#converting object datatypes to category datatype
for feature in visa.columns:
  if visa[feature].dtype=="object":
      visa[feature] = pd.Categorical(visa[feature])
visa.info()

"""All of the "object" datatype columns are now "category" datatype."""

#checking for duplicate values
visa.duplicated().sum()

"""There are no duplicate values in the dataset."""

#checking for missing values
visa.isnull().sum()

"""There are no missing values in the dataset.

## Exploratory Data Analysis (EDA)

- EDA is an important part of any project involving data.
- It is important to investigate and understand the data better before building a model with it.
- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.
- A thorough analysis of the data, in addition to the questions mentioned below, should be done.

**Leading Questions**:
1. Those with higher education may want to travel abroad for a well-paid job. Does education play a role in Visa certification?

2. How does the visa status vary across different continents?

3. Experienced professionals might look abroad for opportunities to improve their lifestyles and career development. Does work experience influence visa status?

4. In the United States, employees are paid at different intervals. Which pay unit is most likely to be certified for a visa?

5. The US government has established a prevailing wage to protect local talent and foreign workers. How does the visa status change with the prevailing wage?

First, we can define functions to create graphs for visualization.
"""

#function to create labeled barplots

def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e. display all levels)
    """

    total = len(data[feature]) #length of column
    count = data[feature].nunique()
    if n is None:
      plt.figure(figsize=(count + 2, 6))
    else:
      plt.figure(figsize=(n +2, 6))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n],
    )

    for p in ax.patches:
      if perc == True:
        label = "{:.1f}%".format(
           100* p.get_height() / total
        ) #percentage of each class of the category
      else:
        label = p.get_height() #count of each level of the category

      x = p.get_x() + p.get_width() / 2 #width of the plot
      y = p.get_height() #height of the plot

      ax.annotate(
        label,
        (x,y),
        ha="center",
        va="center",
        size=12,
        xytext=(0,5),
        textcoords="offset points",
      ) #annotate the percentage

    plt.show()

#function to create combined histogram and boxplot

def histogram_boxplot(data, feature, figsize=(15, 10), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (15,10))
    kde: whether to show the density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a triangle will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram

#function to plot stacked bar chart

def stacked_barplot(data, predictor, target):
    """
    Print the category counts and plot a stacked bar chart

    data: dataframe
    predictor: independent variable
    target: target variable
    """
    count = data[predictor].nunique()
    sorter = data[target].value_counts().index[-1]
    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(
        by=sorter, ascending=False
    )
    print(tab1)
    print("-" * 120)
    tab = pd.crosstab(data[predictor], data[target], normalize="index").sort_values(
        by=sorter, ascending=False
    )
    tab.plot(kind="bar", stacked=True, figsize=(count + 5, 6))
    plt.legend(
        loc="lower left", frameon=False,
    )
    plt.legend(loc="upper left", bbox_to_anchor=(1,1))
    plt.show()

#function to plot distributions wrt target

def distribution_plot_wrt_target(data, predictor, target):

    fig, axs = plt.subplots(2, 2, figsize=(12,10))

    target_uniq = data[target].unique()

    axs[0,0].set_title("Distribution of target for target=" + str(target_uniq[0]))
    sns.histplot(
        data=data[data[target] == target_uniq[0]],
        x=predictor,
        kde=True,
        ax=axs[0,0],
        color="teal",
        stat="density",
    )

    axs[0, 1].set_title("Distribution of target for target=" +str(target_uniq[1]))
    sns.histplot(
        data=data[data[target] == target_uniq[1]],
        x=predictor,
        kde=True,
        ax=axs[0,1],
        color="orange",
        stat="density",
    )

    axs[1,0].set_title("Boxplot w.r.t target")
    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1,0], palette="gist_rainbow")

    axs[1,1].set_title("Boxplot (without outliers) w.r.t target")
    sns.boxplot(
        data=data,
        x=target,
        y=predictor,
        ax=axs[1,1],
        showfliers=False,
        palette="gist_rainbow",
    )

    plt.tight_layout()
    plt.show()

#checking statistical summary of the data
visa.describe(include="all").T

""" - The "case_id" column is a unique identifier and can be removed.
 - The "continent" column has 6 unique values, of which Asia is the most frequent at over 60%.
 - The "education_of_employee" column has 4 unique values with Bachelor's as the most frequent.
 - The "has_job_experience" column has 2 unique values (yes - Y, or no - N), with Y being the most common at over 50%.
 - The "required_job_training" column also has 2 unique values (Y and N) with N
  being the most common at nearly 90%.
 - The "no_of_employees" column has a mean of 5667, with a minimum of -26 (we will need to adjust this with the absolute value) and a maximum of 602,069. The 75th percentile is 3504, indicating that this column is very right-skewed.
 - The "yr_of_estab" column has a minimum of 1800 and a maximum of 2016.
 - The "region_of_employment" has 5 unique values, with northeast being the most frequent.
 - The "prevailing_wage" column has a mean of around 75,000, with a minimum of just over 2 and a maximum of 319,210. The 75th percentile is 107,735, indicating that this column is right-skewed.
 - The "unit_of_wage" column has 4 unique values, with Year being the most frequent at nearly 90%.
 - The "full_time_position" has 2 unique values (Y and N), with Y being the most frequent at nearly 90%.
 - The "case_status" column, the dependent variable, has 2 unique values - Certified and Denied - with Certified being the most common at around 67%.
"""

#removing the case_id column
visa = visa.drop(["case_id"], axis=1)

#fixing the negative values in the no_of_employees column by taking the absolute value
visa["no_of_employees"] = abs(visa["no_of_employees"])

visa.describe().T

"""The minimum number of employees is now 11.

###Univariate Analysis
"""

#observations on number of employees
histogram_boxplot(visa, "no_of_employees")

visa["no_of_employees"].max()

"""  As noted when we looked at the statistical summary, this column is extremely right-skewed, as most of the data falls under 3500 employees. However, the highest number of employees is over 600,000."""

#observations on year established
histogram_boxplot(visa, "yr_of_estab")

"""This column is left skewed, as the majority of the data falls after year 1975. However, the earliest year is 1800."""

#observations on prevailing wage
histogram_boxplot(visa, "prevailing_wage")

"""It appears there is a spike in observations that have a very low wage. After that spike, the data appears to have a slightly bell-shaped curve until around 150,000, where we can see there are a lot of outliers on the higher side, up to over 300,000.

Let's build another histogram that takes into account the wage unit.
"""

sns.histplot(visa, x="prevailing_wage", hue="unit_of_wage")
plt.show()

"""This graph helps us understand that the spike in low wages is largely due to those wages being represented in units of hourly pay instead of yearly pay. Going forward, we will not normalize the column to yearly wage as we do not know if there is a consistent number of working hours between these jobs. However, we can further explore the relationship between this variable and the dependent variable later on in our bivariate analysis.

Let's take a closer look at the unit_of_wage column.
"""

#observations on unit of wage
labeled_barplot(visa, "unit_of_wage", perc=True)

"""90% of the observations had a wage unit of year, with 8.5% as hour. Less than 2% used week or month."""

#observations on continent
labeled_barplot(visa, "continent", perc=True)

"""66% of observations included employees from Asia, followed by 15% from Europe and 13% from North America. Less than 7% of observations included employees from South America, Africa, or Oceania."""

#observations on education of employee
labeled_barplot(visa, "education_of_employee", perc=True)

"""40% of the observations included employees with a Bachelor's Degree, and 38% with a Master's degree. Around 20% of observations included employees with a High School degree or a Doctorate."""

#observations on job experience
labeled_barplot(visa, "has_job_experience", perc=True)

"""58% of employees have job experience and 42% do not."""

#observations on whether the employee requires job training
labeled_barplot(visa, "requires_job_training", perc=True)

"""88% of employees do not require job training and 12% do."""

#observations on region of employment
labeled_barplot(visa, "region_of_employment", perc=True)

"""The Northeast, South and West all have similar prevalence in the dataset (around 26-28%), followed by the Midwest and lastly Island."""

#observations on full time position
labeled_barplot(visa, "full_time_position", perc=True)

"""In 89% of observations, the position is full time. In 11% it is not full time."""

#observations on case status
labeled_barplot(visa, "case_status", perc=True)

"""For our dependent variable, around two thirds (66.8%) of observations were certfied, and around a third (33.2%) were denied.

###Bivariate Analysis

First we will quickly check a correlation map to check the relationship between the 3 continuous variables. However, given that the dependent variable is categorical, and the yr_of_estab column is temporal, it is likely we will not gain much insight through this map.
"""

plt.figure(figsize=(12,7))
sns.heatmap(visa.corr(), annot=True, vmin=-1, vmax=1, fmt=".1g", cmap="Spectral")
plt.show()

"""None of the continuous variables have a notable correlation.

Now we can look at the relationship between the dependent variable and other variables.
"""

stacked_barplot(visa, "case_status", "education_of_employee")

"""Visa status seems to vary with education level to a notable degree. There was a larger proportion of employees holding Master's and Doctorate when the visa was certified, and a lower proportion of employees holding Bachelor's degrees or High School diplomas, as compared to when the visa was denied. (Question 1 above)"""

stacked_barplot(visa, "case_status", "continent")

"""The distribution for case status across continents does not appear to vary much between denied vs certified visas. However there seems to be a slightly higher number of observations from Asia and a slightly lower number of observations from Europe that were denied as opposed to certified. (Question 2 above)"""

stacked_barplot(visa, "case_status", "has_job_experience")

"""A larger proportion of visas were certified when the employee had work experience as opposed to not having any (65% of certified visas included an employee with work experience). (Question 3 above)"""

stacked_barplot(visa, "case_status", "requires_job_training")

"""There is not a notable difference in visa status on the basis of whether the employee requires training."""

distribution_plot_wrt_target(visa, "no_of_employees", "case_status")

"""There does not appear to be a considerable difference in visa status on the basis of the company number of employees. Certified visas appear to include a slightly higher range of number of employees, but this difference is not very notable."""

distribution_plot_wrt_target(visa, "yr_of_estab", "case_status")

"""There does not appear to be a notable difference in visa status on the basis of the year of company establishment."""

stacked_barplot(visa, "region_of_employment", "case_status")

"""The rate of visa certification for the regions Island, West, and Northeast are comparable, and the regions South and Midwest have a slightly higher certification rate."""

distribution_plot_wrt_target(visa, "prevailing_wage", "case_status")

"""There is a slightly higher density of wages under 150,000 for visas that were certified as opposed to denied. However, there is a notably higher density of lower wages that were denied visas (Question 5). Since this spike in low wages was associated with hourly unit pay from our earlier analysis, let's check the relationship between case status and wage unit."""

stacked_barplot(visa, "case_status", "unit_of_wage")

"""There is a notably larger proportion of hourly wage unit for visas that were denied as opposed to certified. Certified visas had a larger proportion of yearly wage unit than denied visas. (Question 4 above)"""

stacked_barplot(visa, "case_status", "full_time_position")

"""There is not a notable difference in visa status between jobs that are full time versus part time.

It appears that unit wage, education of employee, prior job experience, and prevailing wage are 4 of the features most affecting visa status. Let's look at the relationship between unit wage and education.
"""

stacked_barplot(visa, "unit_of_wage", "education_of_employee")

"""Although there doesn't seem to be a large correlation between these two features, employees with a High School diploma have a slightly higher prevalence in jobs with an hourly wage, as compared to the other unit wage jobs. Likewise, employees with  a Master's degree have the highest prevalence in jobs with yearly and weekly wage units, and employees with a Doctorate have the highest prevalence in jobs with yearly wage units. Somewhat surprisingly, employees with Bachelor's have the lowest prevalence in jobs with a yearly wage as compared to other types of wages.

Now let's take a look at prior job experience and prevailing wage.
"""

distribution_plot_wrt_target(visa, "prevailing_wage", "has_job_experience")

"""The histogram shows that there is a slightly higher density of lower wages (within the spike correlated with hourly unit wage) for employees without job experience, as well as a slightly lower density of moderate wage jobs. Interestingly, the boxplot shows that employees without prior job experience actually have a slightly higher range of wages than those with prior experience.

Lastly, let's look at prevailing wage and region.
"""

sns.boxplot(visa, x="region_of_employment", y="prevailing_wage")

"""Wages in the Island and Midwestern regions appear to be the highest on average, with wages in the Northeast being the lowest, followed by the Western and Southern regions. The West and Midwest regions appear to have the largest range of wages, with large outliers.

## Data Preprocessing

- Missing value treatment (if needed)
- Feature engineering
- Outlier detection and treatment (if needed)
- Preparing data for modeling
- Any other preprocessing steps (if needed)

We have noted previously that there are no missing values or duplicate values in the dataset. We can now look at outliers and treat if needed.
"""

#outlier detection using boxplots
num_cols = visa.select_dtypes(include=np.number).columns.tolist()

plt.figure(figsize=(20,15))

for i, variable in enumerate(num_cols):
  plt.subplot(4,4, i+1)
  plt.boxplot(visa[variable], whis=1.5)
  plt.tight_layout()
  plt.title(variable)

plt.show()

"""Since prevailing wage and no_of_employees are very right-skewed, let's look at the number of observations exceeding 1.5*IQR for these features to help us determine if we should treat the outliers."""

#calculating 25th quartile
Q1_wage = visa["prevailing_wage"].quantile(0.25)
#calculate 75th quartile
Q3_wage = visa["prevailing_wage"].quantile(0.75)

IQR_wage = Q3_wage-Q1_wage

#calculating upper whisker
IQR_upper_wage= Q3_wage + 1.5*IQR_wage
IQR_upper_wage

visa[visa["prevailing_wage"]>IQR_upper_wage].count()

"""There are 427 observation in the prevailing wage column that exceed 1.5*upper IQR for this feature"""

#calculating 25th quartile
Q1_employees = visa["no_of_employees"].quantile(0.25)
#calculate 75th quartile
Q3_employees = visa["no_of_employees"].quantile(0.75)

IQR_employees = Q3_employees-Q1_employees

#calculating upper whisker
IQR_upper_employees= Q3_employees + 1.5*IQR_employees
IQR_upper_employees

visa[visa["no_of_employees"]>IQR_upper_employees].count()

"""There are over 1500 observations in the number of employees column that exceed 1.5*upper IQR for this feature.

Given that there are a notable number of outliers in these columns, it is clear that numbers over 1.5*upper IQR for these features are not errors, and represent real values. Additionally, some of the methods we will utilize below such as Random Forest and Bagging Classifier are robust against outliers. Because of all these factors, we will not treat the outliers in these columns and therefore will not lose valuable data.

We will also not treat the outliers in the "year_of_estab" column since these are also real values and represent years, not a quantity.

We can now move forward with prepping the data for modeling.
"""

#encoding the dependent variable
visa["case_status"] = visa["case_status"].apply(lambda x: 1 if x=="Certified" else 0)
visa.head()

#defining the dependent and independent variables
X = visa.drop(["case_status"], axis=1)
Y = visa["case_status"]

#creating dummy variables for X
X = pd.get_dummies(X, columns=X.select_dtypes(include="category").columns.tolist())

#splitting data into train and test set
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1, stratify=Y)
print(X_train.shape, X_test.shape)

X.head()

Y.value_counts(1)

Y_test.value_counts(1)

"""The data has been split properly. The stratify argument when splitting the data maintained the original distribution of classes in the target variable (67% certified, 33% denied).

##Model evaluation criterion

To determine which model evaluation metric is the most important, we can look at the two types of incorrect predictions the model may make:

 1) Predicting that the visa application will get certified, but it should actually get denied (false positive) - This is a problem because an unfit employee may get the job over a US citizen or another foreign employee who would have been certified.

 2) Predicting that the visa application will get denied, but it should actually get certified (false negative) - This is a problem because the US business will lose out on a valuable employee.

 Because both of these errors are important to reduce, we will choose the F1 score as the metric for evaluation of the model, since the F1 score maximizes precision and recall, reducing both false positives and false negatives.

With this in mind, we can create sklearn functions to calculate different model performance metrics as well as the confusion matrix.
"""

# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_perf(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    #predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)
    recall = recall_score(target, pred)
    precision = precision_score(target, pred)
    f1 = f1_score(target, pred)

    #creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1,
        },
        index=[0],
    )

    return df_perf

def confusion_matrix_sklearn(model, predictors, target):
    """
    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm =confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")

"""## Building bagging and boosting models

We will build a few types of bagging models including Decision Tree, Bagging classifier, and Random Forest. Then we will build a few types of boosting models including AdaBoost, GradientBoosting, and XGBoost. We will also build a stacking classifier.

We will start with the bagging models.

**Decision Tree**
"""

#fitting the model
dtree = DecisionTreeClassifier(random_state=1)
dtree.fit(X_train, Y_train)

#calculating performance metrics
dtree_train_perf = model_perf(dtree, X_train, Y_train)
print("Training performance:\n", dtree_train_perf)
dtree_test_perf = model_perf(dtree, X_test, Y_test)
print("Test performance:\n", dtree_test_perf)

#creating confusion matrix for test set
confusion_matrix_sklearn(dtree, X_test, Y_test)

"""The decision tree classifier is performing moderate to well on the test data, however it is strongly overfitting on the training data. Later on, we will tune this model to correct the overfitting.

The confusion matrix shows that this model is not as effective at predicting true negatives compared to true positives. 1265 of the 2539 true negatives (49.8%) were predicted to be positive. 1331 of the 1505 true positives (26%) were predicted to be negatives.

**Bagging Classifier**
"""

#fitting the model
bagging_classifier = BaggingClassifier(random_state=1)
bagging_classifier.fit(X_train,Y_train)

#calculating performance metrics
bagging_classifier_train_perf=model_perf(bagging_classifier, X_train, Y_train)
print("Training performance:\n",bagging_classifier_train_perf)
bagging_classifier_test_perf=model_perf(bagging_classifier, X_test, Y_test)
print("Testing performance:\n",bagging_classifier_test_perf)

#creating confusion matrix on test set
confusion_matrix_sklearn(bagging_classifier, X_test, Y_test)

"""The bagging classifier is performing slightly better on the test data than the decision tree, but is still overfitting on the training data.

**Random Forest**
"""

#fitting the model
rf = RandomForestClassifier(random_state=1)
rf.fit(X_train, Y_train)

#calculating performance metrics
rf_train_perf=model_perf(rf, X_train, Y_train)
print("Training performance:\n", rf_train_perf)
rf_test_perf=model_perf(rf, X_test, Y_test)
print("Testing performance:\n",rf_test_perf)

#creating confusion matrix on test set
confusion_matrix_sklearn(rf, X_test, Y_test)

"""The random forest model is performing similarly to the bagging classifier model on the test data, and this model is also overfitting.

**Adaboost**
"""

#fitting the model
ab_classifier = AdaBoostClassifier(random_state=1)
ab_classifier.fit(X_train, Y_train)

#calculating performance metrics
ab_classifier_train_perf=model_perf(ab_classifier, X_train, Y_train)
print("Training performance:\n", ab_classifier_train_perf)
ab_classifier_test_perf=model_perf(ab_classifier, X_test, Y_test)
print("Testing performance:\n", ab_classifier_test_perf)

#creating confusion matrix on the test set
confusion_matrix_sklearn(ab_classifier, X_test, Y_test)

"""The AdaBoost model is performing similarly to the random forest and bagging classifier models on the test data, with slightly higher recall. However, this model is not overfitting.

**GradientBoost**
"""

#fitting the model
gb_classifier = GradientBoostingClassifier(random_state=1)
gb_classifier.fit(X_train, Y_train)

#calculating performance metrics
gb_classifier_train_perf=model_perf(gb_classifier, X_train, Y_train)
print("Training performance:\n", gb_classifier_train_perf)
gb_classifier_test_perf=model_perf(gb_classifier, X_test, Y_test)
print("Testing performance:\n",gb_classifier_test_perf)

#creating confusion matrix
confusion_matrix_sklearn(gb_classifier, X_test, Y_test)

"""The GradientBoost model is performing similarly to the AdaBoost model on the test data, and is also not overfitting.

**XGBoost**
"""

#fitting the model
xgb = XGBClassifier(random_state=1)
xgb.fit(X_train, Y_train)

#calculating performance metrics
xgb_train_perf=model_perf(xgb, X_train, Y_train)
print("Training performance:\n", xgb_train_perf)
xgb_test_perf=model_perf(xgb, X_test, Y_test)
print("Testing performance:\n",xgb_test_perf)

#creating confusion matrix
confusion_matrix_sklearn(xgb, X_test, Y_test)

"""The XGBoost model is performing similarly to the other two boosting models on the test data, however this model is slightly overfitting on the train data.

##  Will tuning the hyperparameters improve the model performance?

**Tuned Decision Tree**
"""

#choosing decision tree as the type of classifier (including class_weight balancing)
dtree_tuned = DecisionTreeClassifier(class_weight={0:0.67,1:0.33},random_state=1)

#grid of parameters to choose from
parameters = {'max_depth': np.arange(2, 15),
              'min_samples_leaf': [2, 5, 10],
              'max_leaf_nodes' : [2, 5, 10, 15],
              'min_impurity_decrease': [0.0001,0.001,0.01]
             }

#type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.f1_score)

#running the grid search
grid_obj = GridSearchCV(dtree_tuned, parameters, scoring=scorer, n_jobs=-1)
grid_obj = grid_obj.fit(X_train, Y_train)

#setting the classifier to the best combination of parameters
dtree_tuned = grid_obj.best_estimator_

#fitting the best algorithm to the data
dtree_tuned.fit(X_train, Y_train)

#calculating performance metrics
dtree_tuned_train_perf = model_perf(dtree_tuned, X_train, Y_train)
print("Training performance:\n", dtree_tuned_train_perf)
dtree_tuned_test_perf = model_perf(dtree_tuned, X_test, Y_test)
print("Test performance:\n", dtree_tuned_test_perf)

#creating confusion matrix for test set
confusion_matrix_sklearn(dtree_tuned, X_test, Y_test)

"""Tuning has decreased the overfitting of the DecisionTreeClassifier and has increased the each of the metrics on the test data except for precision, which has slightly decreased. Recall has undergone the most notable improvement, increasing by 0.2.

**Tuned Bagging Classifier**
"""

#choosing type of classifier
bagging_tuned = BaggingClassifier(random_state=1)

#grid of parameters to choose from
parameters = {
              'base_estimator': [None, LogisticRegression(solver='liblinear', random_state=1, max_iter=1000)],
              'max_samples': [0.7, 0.8, 0.9],
              'max_features': [0.7, 0.8, 0.9],
              'n_estimators' : [25, 50, 75, 100],
             }

#type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.f1_score)

#running the grid search
grid_obj = GridSearchCV(bagging_tuned, parameters, scoring=scorer, cv=5)
grid_obj = grid_obj.fit(X_train, Y_train)

#setting the classifier to the best combination of parameters
bagging_tuned = grid_obj.best_estimator_

#fitting the best algorithm to the data
bagging_tuned.fit(X_train, Y_train)

#calculating performance metrics
bagging_tuned_train_perf = model_perf(bagging_tuned, X_train, Y_train)
print("Training performance:\n", bagging_tuned_train_perf)
bagging_tuned_test_perf = model_perf(bagging_tuned, X_test, Y_test)
print("Test performance:\n", bagging_tuned_test_perf)

#creating confusion matrix for test set
confusion_matrix_sklearn(bagging_tuned, X_test, Y_test)

"""Surprisingly, hyperparameter tuning has not reduced overfitting for the Bagging Classifier, though accuracy, recall, and F1 scores have improved on test data. Precision has decreased.

**Tuned Random Forest**
"""

#choosing the type of classifier
rf_tuned = RandomForestClassifier(class_weight={0:0.18,1:0.82},random_state=1,oob_score=True,bootstrap=True)

parameters = {
                'max_depth': list(np.arange(5, 20, 5)),
                'max_features': ['sqrt','log2'],
                'min_samples_leaf': np.arange(1, 21, 5),
                'n_estimators': np.arange(10, 110, 20)}


#type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.f1_score)

#running the grid search
grid_obj = GridSearchCV(rf_tuned, parameters, scoring=scorer, cv=5, n_jobs=-1)
grid_obj = grid_obj.fit(X_train, Y_train)

#setting the classifier to the best combination of parameters
rf_tuned = grid_obj.best_estimator_

#fitting the best algorithm to the data.
rf_tuned.fit(X_train, Y_train)

#calculating performance metrics
rf_tuned_train_perf = model_perf(rf_tuned, X_train, Y_train)
print("Training performance:\n", rf_tuned_train_perf)
rf_tuned_test_perf = model_perf(rf_tuned, X_test, Y_test)
print("Testing performance:\n", rf_tuned_test_perf)

#creating confusion matrix for test set
confusion_matrix_sklearn(rf_tuned, X_test, Y_test)

"""After hyperparameter tuning the random forest model, the overfitting has decreased, although there is still some slight overfitting. Test set performance increased on recall quite notably, and F1 has increased slightly. However, precision has reduced slightly.

**Tuned Adaboost**
"""

#choosing the type of classifier
abc_tuned = AdaBoostClassifier(random_state=1)

#grid of parameters to choose from
parameters = {
    #trying different max_depth for base_estimator as well as balancing the class weights
    "base_estimator":[DecisionTreeClassifier(max_depth=1, class_weight="balanced", random_state=1),
                      DecisionTreeClassifier(max_depth=2, class_weight="balanced", random_state=1),
                      DecisionTreeClassifier(max_depth=3, class_weight="balanced", random_state=1)],
    "n_estimators": np.arange(50, 110, 10),
    "learning_rate":np.arange(0.1, 1, 0.1)
}

#type of scoring used to compare parameter  combinations
scorer = metrics.make_scorer(metrics.f1_score)

#running the grid search
grid_obj = GridSearchCV(abc_tuned, parameters, scoring=scorer, cv=5)
grid_obj = grid_obj.fit(X_train, Y_train)

#setting the classifier to the best combination of parameters
abc_tuned = grid_obj.best_estimator_

#fitting the best algorithm to the data
abc_tuned.fit(X_train, Y_train)

#calculating performance metrics
abc_tuned_train_perf = model_perf(abc_tuned, X_train, Y_train)
print("Training performance:\n", abc_tuned_train_perf)
abc_tuned_test_perf = model_perf(abc_tuned, X_test, Y_test)
print("Testing performance:\n", abc_tuned_test_perf)

#creating confusion matrix for test set
confusion_matrix_sklearn(abc_tuned, X_test, Y_test)

"""Interestingly, the tuned AdaBoost classifier is performing slightly worse on the test data than the untuned model, with the exception of precision increasing slightly. However, this model is also not overfitting.

**Tuned GradientBoost**
"""

#choosing the type of classifier
gbc_tuned = GradientBoostingClassifier(init=AdaBoostClassifier(random_state=1),random_state=1)

#grid of parameters to choose from
parameters = {
    "n_estimators": [100, 150, 200, 250],
    "subsample":[0.8, 0.9, 1],
    "learning_rate": [0.1, 0.31, 0.1],
    "max_features":[0.7, 0.9],
}

#type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.f1_score)

#running the grid search
grid_obj = GridSearchCV(gbc_tuned, parameters, scoring=scorer, cv=5)
grid_obj = grid_obj.fit(X_train, Y_train)

#setting the classifier to the best combination of parameters
gbc_tuned = grid_obj.best_estimator_

#fitting the best algorithm to the data.
gbc_tuned.fit(X_train, Y_train)

#calculating performance metrics
gbc_tuned_train_perf = model_perf(gbc_tuned, X_train, Y_train)
print("Training performance:\n", gbc_tuned_train_perf)
gbc_tuned_test_perf = model_perf(gbc_tuned, X_test, Y_test)
print("Testing performance:\n", gbc_tuned_test_perf)

#creating confusion matrix for test set
confusion_matrix_sklearn(gbc_tuned, X_test, Y_test)

"""Hyperparameter tuning the GradientBoosting model did not improve the models performance, and the model is still  not overfitting.

**Tuned XGBoost**
"""

#choosing the type of classifier
xgb_tuned = XGBClassifier(random_state=1, eval_metric='logloss')

# Grid of parameters to choose from
parameters = {
    "n_estimators": [100, 200],
    "scale_pos_weight":[2, 5],
    "subsample":[0.8, 1],
    "learning_rate":[0.1, 0.2],
    "colsample_bytree":[0.7, 0.9],
    "gamma": [2, 5]
}

#type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.f1_score)

#running the grid search
grid_obj = GridSearchCV(xgb_tuned, parameters,scoring=scorer, cv=5)
grid_obj = grid_obj.fit(X_train, Y_train)

#setting the clf to the best combination of parameters
xgb_tuned = grid_obj.best_estimator_

#fitting the best algorithm to the data.
xgb_tuned.fit(X_train, Y_train)

#calculating performance metrics
xgb_tuned_train_perf = model_perf(xgb_tuned, X_train, Y_train)
print("Training performance:\n", xgb_tuned_train_perf)
xgb_tuned_test_perf = model_perf(xgb_tuned, X_test, Y_test)
print("Testing performance:\n", xgb_tuned_test_perf)

#creating confusion matrix for test set
confusion_matrix_sklearn(xgb_tuned, X_test, Y_test)

"""After hyperparameter tuning, performance on accuracy and precision has decreased on the test set, while recall has increased and f1 score has stayed nearly the same. Overfitting on the train set has been reduced.

**Stacking Classifier**
"""

estimators = [('Gradient Boosting', gbc_tuned), ("AdaBoosting", ab_classifier), ('Random Forest', rf_tuned)]

final_estimator = xgb_tuned

stacking_classifier= StackingClassifier(estimators=estimators, final_estimator=final_estimator)

stacking_classifier.fit(X_train,Y_train)

#calculating performance metrics
stacking_train_perf = model_perf(stacking_classifier, X_train, Y_train)
print("Training performance:\n", stacking_train_perf)
stacking_test_perf = model_perf(stacking_classifier, X_test, Y_test)
print("Testing performance:\n", stacking_test_perf)

#creating confusion matrix for test set
confusion_matrix_sklearn(stacking_classifier, X_test, Y_test)

"""The stacking classifier is giving a similar performance as the tuned XGBoost model, but with very slightly less overfitting. The confusion matrix shows that the model is better at identifying true positives, as only 2.7% of predictions were false negatives (4% of all true positives).

## Model Performance Comparison and Conclusions
"""

#training performance comparison
models_train_comp_df = pd.concat(
    [dtree_train_perf.T, dtree_tuned_train_perf.T, bagging_classifier_train_perf.T, bagging_tuned_train_perf.T, rf_train_perf.T, rf_tuned_train_perf.T,
     ab_classifier_train_perf.T, abc_tuned_train_perf.T, gb_classifier_train_perf.T,gbc_tuned_train_perf.T, xgb_train_perf.T,
    xgb_tuned_train_perf.T, stacking_train_perf.T],
    axis=1,
)
models_train_comp_df.columns = [
    "Decision Tree",
    "Decision Tree Estimator",
    "Bagging Classifier",
    "Bagging Estimator Tuned",
    "Random Forest Estimator",
    "Random Forest Tuned",
    "Adaboost Classifier",
    "Adaboost Classifier Tuned",
    "Gradient Boost Classifier",
    "Gradient Boost Classifier Tuned",
    "XGBoost Classifier",
    "XGBoost Classifier Tuned",
    "Stacking Classifier"]
print("Training performance comparison:")
models_train_comp_df

"""On the train data, the Decision Tree, Random Forest, and Tuned Bagging Estimator had the highest F1 scores. However, the majority of these models are overfitting on the training data."""

# testing performance comparison
models_test_comp_df = pd.concat(
    [dtree_test_perf.T, dtree_tuned_test_perf.T, bagging_classifier_test_perf.T, bagging_tuned_test_perf.T, rf_test_perf.T, rf_tuned_test_perf.T,
     ab_classifier_test_perf.T, abc_tuned_test_perf.T, gb_classifier_test_perf.T, gbc_tuned_test_perf.T, xgb_test_perf.T,
    xgb_tuned_test_perf.T, stacking_test_perf.T],
    axis=1,
)
models_test_comp_df.columns = [
    "Decision Tree",
    "Decision Tree Estimator",
    "Bagging Classifier",
    "Bagging Estimator Tuned",
    "Random Forest Estimator",
    "Random Forest Tuned",
    "Adaboost Classifier",
    "Adabosst Classifier Tuned",
    "Gradient Boost Classifier",
    "Gradient Boost Classifier Tuned",
    "XGBoost Classifier",
    "XGBoost Classifier Tuned",
    "Stacking Classifier"]
print("Testing performance comparison:")
models_test_comp_df

"""On the test data, respectively, the Tuned GradientBoost Classifier, Untuned GradientBoost Classifier, the Stacking Classifier, and the Tuned XGBoost Classifier had the highest F1 scores. We can choose the Tuned GradientBoost Classifier as our final model, since this model has the highest F1 score on the test data, has generalized good performance, and is also not overfitting on the train data.

##Feature Importances

Let's look at the most important features of the final model
"""

feature_names = X_train.columns
importances = gbc_tuned.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12,12))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='violet', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()

"""Employee education, job experience, and job unit wage are the three most important features according to the Tuned GradientBoosting Classifier model. As the unit of wage was highly correlated with prevailing wage, the next most important feature, we can consider that feature to also be one of the most important predictors for visa certification.

## Actionable Insights and Recommendations

- Based on our analysis, we can say that visas that get certified have the following features in comparison to visas that do not get certified:

  - more likely to have a master's degree or doctorate degree
  -  more likely to be from Europe, and less likely to be from North America or Asia (note: the majority of visas that get certified are for employees from Asia, because those employees make up the majority of observations. However, the proportion of applications for Asian employees that are certified is lower than those that are denied)
  - more likely to be applying to a job in the midwestern or southern region of the US
  - more likely to have job experience
  - more likely to have a moderate prevailing wage
  - more likely to have a yearly salary and less likely to have an hourly salary

- Employee education, job experience, job unit wage, and prevailing wage are the top most important features to predict whether a visa will get certified. Note that these 4 features were also concluded to likely be the most highly correlated with the dependent variable in our bivariate analysis.
- The company should pay most attention to the above features when shortlisting candidates. For example, the ideal candidate profile would include an employee with a master's degree or doctorate that has job experience, and is applying to a job with a moderate to high prevailing wage and a yearly salary. Applicants with 3 to 4 of these preferable features have the highest chance of having a visa certfied.
  - If applicants do not have all these preferable features, it could be helpful to look at the remaining features and their likelihood to lead to certification. For example, an employee with a Master's degree but with no job experience may have a higher likelihood of visa certification if they are applying to a job in the midwestern or southern region of the US.
- A candidate that is likely to be denied and should not be shortlisted may have a high school degree and no job experience, with a low hourly wage. Even if a candidate has 2 or 3 of these features, it may notably reduce the chance of the visa being certified.
"""