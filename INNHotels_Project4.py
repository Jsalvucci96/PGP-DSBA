# -*- coding: utf-8 -*-
"""Copy of Project_SLC_DSBA_INNHotels_FullCode - Joanna Salvucci.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g0TMGzLHnuHVnCRttePPY3Hl3YnQvbxP

# INN Hotels Project

## Context

A significant number of hotel bookings are called-off due to cancellations or no-shows. The typical reasons for cancellations include change of plans, scheduling conflicts, etc. This is often made easier by the option to do so free of charge or preferably at a low cost which is beneficial to hotel guests but it is a less desirable and possibly revenue-diminishing factor for hotels to deal with. Such losses are particularly high on last-minute cancellations.

The new technologies involving online booking channels have dramatically changed customers’ booking possibilities and behavior. This adds a further dimension to the challenge of how hotels handle cancellations, which are no longer limited to traditional booking and guest characteristics.

The cancellation of bookings impact a hotel on various fronts:
* Loss of resources (revenue) when the hotel cannot resell the room.
* Additional costs of distribution channels by increasing commissions or paying for publicity to help sell these rooms.
* Lowering prices last minute, so the hotel can resell a room, resulting in reducing the profit margin.
* Human resources to make arrangements for the guests.

## Objective
The increasing number of cancellations calls for a Machine Learning based solution that can help in predicting which booking is likely to be canceled. INN Hotels Group has a chain of hotels in Portugal, they are facing problems with the high number of booking cancellations and have reached out to your firm for data-driven solutions. You as a data scientist have to analyze the data provided to find which factors have a high influence on booking cancellations, build a predictive model that can predict which booking is going to be canceled in advance, and help in formulating profitable policies for cancellations and refunds.

## Data Description
The data contains the different attributes of customers' booking details. The detailed data dictionary is given below.


**Data Dictionary**

* Booking_ID: unique identifier of each booking
* no_of_adults: Number of adults
* no_of_children: Number of Children
* no_of_weekend_nights: Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel
* no_of_week_nights: Number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel
* type_of_meal_plan: Type of meal plan booked by the customer:
    * Not Selected – No meal plan selected
    * Meal Plan 1 – Breakfast
    * Meal Plan 2 – Half board (breakfast and one other meal)
    * Meal Plan 3 – Full board (breakfast, lunch, and dinner)
* required_car_parking_space: Does the customer require a car parking space? (0 - No, 1- Yes)
* room_type_reserved: Type of room reserved by the customer. The values are ciphered (encoded) by INN Hotels.
* lead_time: Number of days between the date of booking and the arrival date
* arrival_year: Year of arrival date
* arrival_month: Month of arrival date
* arrival_date: Date of the month
* market_segment_type: Market segment designation.
* repeated_guest: Is the customer a repeated guest? (0 - No, 1- Yes)
* no_of_previous_cancellations: Number of previous bookings that were canceled by the customer prior to the current booking
* no_of_previous_bookings_not_canceled: Number of previous bookings not canceled by the customer prior to the current booking
* avg_price_per_room: Average price per day of the reservation; prices of the rooms are dynamic. (in euros)
* no_of_special_requests: Total number of special requests made by the customer (e.g. high floor, view from the room, etc)
* booking_status: Flag indicating if the booking was canceled or not.

## Importing necessary libraries and data
"""

#suppress warnings/deprecation notes
import warnings
warnings.filterwarnings("ignore")

#libraries for reading and manipulating data
import numpy as np
import pandas as pd

#libraries for visualization
import matplotlib.pyplot as plt
import seaborn as sns

#library to split data
from sklearn.model_selection import train_test_split

#Limit for number of displayed columns and rows
pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", 200)

#libraries to build logistic regression model
import statsmodels.stats.api as sms
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm
from statsmodels.tools.tools import add_constant
from sklearn.linear_model import LogisticRegression

#To obtain metric scores for logistic regression
from sklearn import metrics
from sklearn.metrics import (
    f1_score,
    accuracy_score,
    recall_score,
    precision_score,
    roc_curve,
    confusion_matrix,
    roc_auc_score,
    precision_recall_curve)

#library to build decision tree classifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

#To tune decision tree
from sklearn.model_selection import GridSearchCV

#Perform statistical analysis
import scipy.stats as stats

#To obtain metric scores for decision tree
from sklearn.metrics import(
    f1_score,
    accuracy_score,
    recall_score,
    precision_score,
    confusion_matrix,
    ConfusionMatrixDisplay,
    make_scorer,
)

#mounting google drive and loading dataset
from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/pgp_module4/week2/INNHotelsGroup.csv')

#make copy to avoid changes to original dataframe
hotels = data.copy()

"""## Data Overview

- Observations
- Sanity checks
"""

#check first and last 5 rows of dataset

hotels.head()

hotels.tail()

"""The dataset has been loaded properly."""

hotels.shape

"""The dataset has 36,275 rows and 19 columns."""

hotels.info()

"""The columns Booking_ID, type_of_meal_plan, room_type_reserved, market_segment_type, and booking_status are object datatypes. The remainder are numerical data types (float or integer). Note that the booking_status column (the dependent variable) is an object datatype."""

hotels.isnull().sum()

"""There are no missing values in the dataframe."""

hotels.duplicated().sum()

"""There are no duplicated values in the dataframe.

## Exploratory Data Analysis (EDA)

- EDA is an important part of any project involving data.
- It is important to investigate and understand the data better before building a model with it.
- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.
- A thorough analysis of the data, in addition to the questions mentioned below, should be done.
"""

#get statistical summary of the dataset
hotels.describe(include="all").T

"""**Observations -**
 - The 25th, 50th, and 75th percentile for number of children is 0, however the maximum is 10. This suggests the no_of_children column is heavily right skewed. The number of adults ranges from 0-4 with an average of around 2.
 - The number of weekend night and number of week nights columns appears to be right skewed as well. The median of weekend nights is 1 and the maximum is 7, and the median of week nights is 2 and the maximum is 17.
 - Meal plan 1 is the most popular with 27,835 bookings choosing that plan, or around 77% of bookings.
 - Up to the 75th percentile, the number of required car parking spaces is 0. This variables has two options - 0 (No - does not require parking space) and 1 (Yes - does require parking space).
 - Room Type 1 is the most popular room type with 28,310 bookings choosing that room type, or around 78%.
 - The lead_time column appears to be right-skewed as well, with a median of 57 days between booking and arrival date, and a maximum of 443 days.
 - 23,214 bookings have a market_segment_type of Online, or 64% of the bookings.
 - The bookings range from 2017 to 2018 and occur in every month and date.
 - Most guests are not repeated guests, as up to the 75th percentile of the repeated_guest column is 0 (No) and the maximum is 1 (Yes).
 - no_of_previous_cancellations also appears to be right skewed, as up to the 75th percentile is 0 cancellations, with the maximum being 13. no_of_previous_bookings_not_canceled includes the 75th percentile as 0 not-cancelled bookings, with the maximum as 58.
 - The price of a room ranged from 0 to 540 euros, with a median of 99 euros.
 - The number of special requests appears right skewed as the median is , the 75th percentile is 1, and the maximum is 5.
 - Booking_status, the dependent variable, includes 24,390 Not Canceled bookings, or around 67%.

**Leading Questions**:
1. What are the busiest months in the hotel?
2. Which market segment do most of the guests come from?
3. Hotel rates are dynamic and change according to demand and customer demographics. What are the differences in room prices in different market segments?
4. What percentage of bookings are canceled?
5. Repeating guests are the guests who stay in the hotel often and are important to brand equity. What percentage of repeating guests cancel?
6. Many guests have special requirements when booking a hotel room. Do these requirements affect booking cancellation?
"""

#function to create labeled barplots

def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e. display all levels)
    """

    total = len(data[feature]) #length of column
    count = data[feature].nunique()
    if n is None:
      plt.figure(figsize=(count + 2, 6))
    else:
      plt.figure(figsize=(n +2, 6))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n],
    )

    for p in ax.patches:
      if perc == True:
        label = "{:.1f}%".format(
           100* p.get_height() / total
        ) #percentage of each class of the category
      else:
        label = p.get_height() #count of each level of the category

      x = p.get_x() + p.get_width() / 2 #width of the plot
      y = p.get_height() #height of the plot

      ax.annotate(
        label,
        (x,y),
        ha="center",
        va="center",
        size=12,
        xytext=(0,5),
        textcoords="offset points",
      ) #annotate the percentage

    plt.show()

#function to plot combined histogram boxplot

def histogram_boxplot(data, feature, figsize=(15,10), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure(default (15,10))
    kde: whether to show the density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2, #number of rows of the subplot grid=2
        sharex=True, #x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    ) #creating 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    ) #boxplot will be created and a triange will indiacte the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    ) #for histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    ) #add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    ) #add median to the histogram

#function to plot stacked bar chart

def stacked_barplot(data, predictor, target):
    """
    Print the category counts and plot a stacked bar chart

    data: dataframe
    predictor: independent variable
    target: target variable
    """
    count = data[predictor].nunique()
    sorter = data[target].value_counts().index[-1]
    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(
        by=sorter, ascending=False
    )
    print(tab1)
    print("-" * 120)
    tab = pd.crosstab(data[predictor], data[target], normalize="index").sort_values(
        by=sorter, ascending=False
    )
    tab.plot(kind="bar", stacked=True, figsize=(count + 5, 6))
    plt.legend(
        loc="lower left", frameon=False,
    )
    plt.legend(loc="upper left", bbox_to_anchor=(1,1))
    plt.show()

#function to plot distributions wrt target

def distribution_plot_wrt_target(data, predictor, target):

    fig, axs = plt.subplots(2, 2, figsize=(12,10))

    target_uniq = data[target].unique()

    axs[0,0].set_title("Distribution of target for target=" + str(target_uniq[0]))
    sns.histplot(
        data=data[data[target] == target_uniq[0]],
        x=predictor,
        kde=True,
        ax=axs[0,0],
        color="teal",
        stat="density",
    )

    axs[0, 1].set_title("Distribution of target for target=" +str(target_uniq[1]))
    sns.histplot(
        data=data[data[target] == target_uniq[1]],
        x=predictor,
        kde=True,
        ax=axs[0,1],
        color="orange",
        stat="density",
    )

    axs[1,0].set_title("Boxplot w.r.t target")
    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1,0], palette="gist_rainbow")

    axs[1,1].set_title("Boxplot (without outliers) w.r.t target")
    sns.boxplot(
        data=data,
        x=target,
        y=predictor,
        ax=axs[1,1],
        showfliers=False,
        palette="gist_rainbow",
    )

    plt.tight_layout()
    plt.show()

"""**Univariate Analysis**"""

#observations on booking_status (our dependent variable)
labeled_barplot(hotels, "booking_status", perc=True)

"""32.8% of bookings in this dataset were canceled (Question 4 above)."""

#observations on no_of_adults
labeled_barplot(hotels, "no_of_adults", perc=True)

"""In 72% of bookings, there are 2 adults, followed by 21% with 1 adult, and 6.4% with 3. 0.4% of bookings have 0 adults."""

#observations on no_of_children
labeled_barplot(hotels, "no_of_children", perc=True)

"""~93% of bookings have 0 children, followed by 4.5% of bookings with 1 child, and 3% with 2 children. Only 0.1% of bookings have 3 children.

We can combine "9" and "10" children with "3" since there are 0 bookings that include 9 and 10 children.
"""

hotels["no_of_children"] = hotels["no_of_children"].replace([9,10], 3)

#observations on no_of_weekend_nights
labeled_barplot(hotels, "no_of_weekend_nights", perc=True)

"""46.5% of bookings do not include any weekend nights, and over 50% do include at least 1 weekend night. 25% of bookings include 2 weekend nights."""

#observations on no_of_week_nights
labeled_barplot(hotels, "no_of_week_nights", perc=True)

"""31.5% of bookings include 2 week nights, and 26.2% of bookings include 1 week night. Around 7% have 0 week nights. Less than 40% of the bookings include more than 2 week nights."""

#observations on type_of_meal_plan
labeled_barplot(hotels, "type_of_meal_plan", perc=True)

"""~77% of bookings use Meal Plan 1, followed by 14% of bookings with no meal plan selected and 9% of bookings with Meal plan 2. None of the bookings included Meal plan 3."""

#observations on required_car_parking_space
labeled_barplot(hotels, "required_car_parking_space", perc=True)

"""96.9% of bookings did not require a car parking space."""

#observations on room_type_reserved
labeled_barplot(hotels, "room_type_reserved", perc=True)

"""77.5% of bookings included Room Type 1, and 16.7% of bookings included Room Type 4. Less than 6% of bookings included Room types 6, 2, 5, 7, or 3."""

#observations on lead_time
histogram_boxplot(hotels, "lead_time")

""" The mean number of days between the day of booking and the arrival date is around 90 and the median is around 55. As seen on both the boxplot and the histogram, the distribution is very right skewed and has many outliers on the higher side."""

#observations on arrival_year
labeled_barplot(hotels, "arrival_year", perc=True)

"""As we noted before, the bookings in our dataset took place in 2017 and 2018. 82% of the bookings took place in 2018."""

#observations on arrival_month
labeled_barplot(hotels, "arrival_month", perc=True)

"""The most popular months for bookings were October (15%), September (13%), and August (11%) (Question 1 above). The least popular months for bookings were January (3%), February (5%), and March (7%)."""

#observations on arrival_date
labeled_barplot(hotels, "arrival_date", perc=True)

"""There does not appear to be much of a pattern for whether bookings are more popular during a certain part of the month."""

#observations on market_segment_type
labeled_barplot(hotels, "market_segment_type", perc=True)

"""The "Online" market segment designation was the most common with 64% of bookings (Question 2 above), followed by 29% with "Offline" market designation. "Aviation" had the lowest number of bookings, with 0.3%."""

#observations on repeated_guest
labeled_barplot(hotels, "repeated_guest", perc=True)

"""97.4% of guests that booked rooms were not repeated guests."""

#observations on no_of_previous_cancellations
labeled_barplot(hotels, "no_of_previous_cancellations", perc=True)

"""The majority (99.1%) of guests that booked rooms did not have previous cancellations. 0.5% had 1 previous cancellation."""

#observations on previous_bookings_not_canceled
histogram_boxplot(hotels, "no_of_previous_bookings_not_canceled")

"""The mean and median of number of previous bookings not canceled is 0 (as most guests were not repeated customers). However, there are some outliers as high as 30+ previous bookings."""

#observations on avg_price_per_room
histogram_boxplot(hotels, "avg_price_per_room")

"""Average price per room appears to be a fairly normal distribution with a median and mean a little over 100 euros. There are outliers on both sides of the boxplot, particularly on the higher side.

There appear to be some rooms that were 0 euros.

As there is one outlier point on the very far side past the 500 euro mark, let's check to see how many bookings in our dataset were over 500 euros.
"""

hotels[hotels["avg_price_per_room"]>=500]

"""We will treat this outlier later on by replacing it with the upper whisker of the IQR."""

#observations on no_of_special_requests
labeled_barplot(hotels, "no_of_special_requests", perc=True)

"""The majority (54.5%) bookings did not include a special request. 31.4% of bookings included 1 special request, and 12% included 2. Less than 3% of bookings included more than 2 special requests.

**Bivariate Analysis**

Before we continue, we will need to encode the dependent variable "booking_status" to be 0 for not canceled and 1 for canceled.
"""

hotels["booking_status"] = hotels["booking_status"].apply(lambda x: 1 if x == "Canceled" else 0)

#heatmap to look at correlation between all numerical attributes
num_cols = hotels.select_dtypes(include=np.number).columns.tolist() #define list of numerical columns

plt.figure(figsize=(15,10))
sns.heatmap(hotels[num_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral")
plt.show()

"""Most of the columns do not have high correlations with other columns. The most highly correlated attribute with the dependent variable, booking status, is lead time with a moderately positive correlation of 0.44. Repeated_guest is moderately positively correlated with no_of_previous_bookings_not_canceled as well as no_of_previous_cancellations.

Let's see how different attributes affect booking status, starting with the number of special requests (Question 6 above).
"""

stacked_barplot(hotels, "no_of_special_requests", "booking_status")

"""From the chart above, it appears that the more special requests a booking has, the less likely it is to be canceled.

Now let's check how booking status is affected by whether or not the guest is a repeated guest.
"""

repeated_guests = hotels[hotels["repeated_guest"]==1]
repeated_guests["booking_status"].value_counts(normalize=True)

"""Of repeated guests, 98.3% did not cancel (Question 5 above).

Now let's check the same for first time guests.
"""

first_time_guests = hotels[hotels["repeated_guest"]==0]
first_time_guests["booking_status"].value_counts(normalize=True)

"""Of first time guests, 66% did not cancel. While we have a much smaller sample size of repeated guests, this is a notable discrepancy in cancellation rate between first time guests and repeated guests"""

#checking booking status vs lead time
distribution_plot_wrt_target(hotels, "lead_time", "booking_status")

"""Lead times appear to be higher when booking status is canceled as opposed to not canceled. However, there appear to be more outliers of higher lead time for bookings that were not canceled."""

#market segment type vs booking status
stacked_barplot(hotels, "market_segment_type", "booking_status")

"""The most amount of cancellations happen in the "Online" market segment, followed by "Offline". "Corporate" has a small proportion of cancellations, and "Complimentary" has none.

Let's look at the difference in price between different market segments (Question 3 above).
"""

#average price of room versus market segment type
sns.boxplot(data=hotels, x="avg_price_per_room", y="market_segment_type")

hotels.groupby(["market_segment_type"])["avg_price_per_room"].mean()

"""The "Online" market segment type had the highest average price per room, followed by "Aviation", "Offline", "Corporate", and finally "Complimentary", which had an average price per room of 3 euros.

Since we know there were some rooms that were free, let's look at the market segment type for when the average price per room was 0.
"""

hotels.loc[hotels["avg_price_per_room"]==0, "market_segment_type"].value_counts()

354 + 191

"""Of the 545 free hotel rooms, 354 were of the "Complementary" and 191 were of the "Online" marketing segment type."""

#average price per room vs booking status
distribution_plot_wrt_target(hotels, "avg_price_per_room", "booking_status")

"""Average price per room tends to be slightly higher for bookings that were canceled. There appears to be a spike in cancellations for bookings around 125 euros."""

#nights stayed vs booking status
new_df = hotels.copy() #creating new dataframe
new_df["no_of_nights"] = new_df["no_of_week_nights"] + new_df["no_of_weekend_nights"] #creating new column in dataframe that includes the total number of nights

distribution_plot_wrt_target(new_df, "no_of_nights", "booking_status")

"""There is not a very large difference in distribution of number of nights stayed for canceled versus not canceled bookings."""

#no_of_children versus booking status
stacked_barplot(hotels, "no_of_children", "booking_status")

"""Bookings with 2 children have the highest likelihood of cancellation, while 1 and 0 children are almost equal in cancellation rate. Bookings with 3 children are the least likely to be canceled."""

#no_of_adults versus booking status
stacked_barplot(hotels, "no_of_adults", "booking_status")

"""Bookings with 3 adults have the highest likelihood of being canceled, however interestingly, bookings with 4 adults have the lowest likelihood of being canceled. 2, 0, and 1 adults in the booking fall in the middle, respectively.

As noted previously, overall the "Online" market segment appears to have the highest average price per room, followed by offline. Corporate and Complimentary have the lowest average prices.
"""

#month versus booking status
sns.lineplot(data=hotels, x="arrival_month", y="booking_status")

"""There seems to be a spike in cancellations around July, and another small spike in October. December and January have the least amount of cancellations."""

#average price per room versus month
sns.lineplot(data=hotels, x="arrival_month", y="avg_price_per_room")

"""Prices start low in the beginning of the year, and rise by April, peaking in September, and falling in December. This matches our earlier analysis of the busiest and least busy times of the year.

## Data Preprocessing

- Missing value treatment (if needed)
- Feature engineering (if needed)
- Outlier detection and treatment (if needed)
- Preparing data for modeling
- Any other preprocessing steps (if needed)

**Outlier Detection**
"""

#outlier detection using boxplots
plt.figure(figsize=(15,10))

for i, variable in enumerate(num_cols):
  plt.subplot(4,4, i+1)
  plt.boxplot(hotels[variable], whis=1.5)
  plt.tight_layout()
  plt.title(variable)

plt.show()

"""We will now treat the outlier in the average price per room column by calculating the IQR and replacing the outlier value with the upper whisker."""

#calculate 25th quantile
Q1 = hotels["avg_price_per_room"].quantile(0.25)

#calculate 75th percentile
Q3 = hotels["avg_price_per_room"].quantile(0.75)

IQR = Q3-Q1

#calculating value of upper whisker
IQR_upper = Q3 +1.5*IQR

#treat outlier in "avg_price_per_room" column
hotels.loc[hotels["avg_price_per_room"]>= 500, "avg_price_per_room"] = IQR_upper

"""We will not treat outliers in any of the other columns, as they are actual data points that represent valuable information that we do not want to remove."""

#Remove "Booking_ID" column as it is a unique identifer and will not help with our analysis

hotels.drop(["Booking_ID"], axis=1, inplace=True)

#convert columns with "object" datatype into categorical variables
for feature in hotels.columns:
  if hotels[feature].dtype=="object":
    hotels[feature] = pd.Categorical(hotels[feature])
hotels.info()

"""The "object" columns have been properly converted to "category."

**Prepare Data for Modeling**
"""

#separating independent and dependent variables
X = hotels.drop(["booking_status"], axis=1)
Y = hotels["booking_status"]

#adding a constant to the independent variables
X = sm.add_constant(X)

#create dummy variables
X = pd.get_dummies(X, drop_first=True)

#splitting data in train and test sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=1)

#check shape and distribution of training and test sets
print("Shape of Training set: ", X_train.shape)
print("Shape of Test set: ", X_test.shape)
print("\nPercentage of classes in training set:")
print(Y_train.value_counts(normalize=True))
print("\nPercentage of classes in test set:")
print(Y_test.value_counts(normalize=True))

"""Based on the numbers shown above, the training set is 70% of the total number of rows and the test set is 30%, showing the data was split into a 70:30 ratio properly. Additionally, the percentage of classes in the train and test set are comparable (around 67% not canceled and around 33% canceled). The data has been encoded and split properly.

## EDA

- It is a good idea to explore the data once again after manipulating it.

To do this, we will double check the correlation map as well as the two columns we made changes to ("avg_price_per_room" and "no_of_children") and their relationship to the dependent variable (if checked before modification).
"""

#rechecking heatmap
plt.figure(figsize=(15,10))
sns.heatmap(hotels[num_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral")
plt.show()

"""The correlation map looks nearly identical to the original."""

#rechecking average price per room
histogram_boxplot(hotels, "avg_price_per_room")

distribution_plot_wrt_target(hotels, "avg_price_per_room", "booking_status")

"""The graphs look very similar to before, except that now the outlier in question has been treated."""

labeled_barplot(hotels, "no_of_children", perc=True)

"""The graph is identical as before except that the 9 and 10 children designation is gone.

Now that we have analyzed our data, we can continue with the process of logistic regression.

## Checking Multicollinearity

- In order to make statistical inferences from a logistic regression model, it is important to ensure that there is no multicollinearity present in the data.

We will check the Variance Inflation Factor (VIF) of each of the predictors to check for multicollinearity. Typically, a VIF value of over 5 means that the column has moderate multicollinearity, and a VIF value of over 10 means that the column has high multicollinearity. We can ignore the VIF value of the constant.
"""

vif_series = pd.Series(
    [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])],
    index=X_train.columns,
    dtype=float
)
print("VIF values: \n\n{}\n".format(vif_series))

hotels["market_segment_type"].unique()

"""We see that the only columns with a VIF greater than 5 are the constant and the categorical dummy variables for market_segment_type Online, Offline, and Corporate. We can ignore these values because dummy variables will usually have some level of multicollinearity within their column (Note that the "Aviation" variable was already removed when we created the dummy variables).

Because all other VIF values are under 5, we do not need to treat multicollinearity in the data. We can proceed to the next step of building a logistic regression model.

## Building a Logistic Regression model
"""

#fitting logistic regression model
model1 = sm.Logit(Y_train, X_train.astype(float))
lg = model1.fit(disp=False)

#print summary
lg.summary()

"""**Removing high p-value variables**

In order to determine which attributes are most important to the model, we will look at their p-values from the summary above. We see that there are several p-values over 0.05 (our chosen level of significance), and so we will drop these attributes one at a time, rebuilding
a model and rechecking the p-values each time (as p-values can change after removing an attribute). We will define a function to make this easier.
"""

#initial list of columns
cols = X_train.columns.tolist()

#set initial max p-value
max_p_value=1

while len(cols) > 0:
  #define train set
  X_train_aux = X_train[cols]

  #fitting the model
  model = sm.Logit(Y_train, X_train_aux).fit(disp=False)

  #rechecking p-values and maximum p-value
  p_values = model.pvalues
  max_p_value = max(p_values)

  #name of variable with max p-value
  feature_with_p_max = p_values.idxmax()

  #removing variable with max p-value if over 0.05
  if max_p_value > 0.05:
    cols.remove(feature_with_p_max)
  else:
    break

#printing columns remaining
selected_features = cols
print(selected_features)

#defining our X_train with the selected features and rebuilding a model
X_train2 = X_train[selected_features]

model2 = sm.Logit(Y_train, X_train2.astype(float))
lg2 = model2.fit(disp=False)

print(lg2.summary())

"""Now no p-values in our model are over 0.05, so we can consider the features included in X_train2 as the final ones and lg2 as our final model.

**Converting coefficients to odds**

Because the coefficients of the logistic regression are in terms of log(odds), we will take the exponential of the coefficients to find the odds.
"""

#converting coefficients to odds
odds = np.exp(lg2.params)

#finding the percentage change
perc_change_odds = (np.exp(lg2.params) - 1)*100

#removing limit from number of columns to display
pd.set_option("display.max_columns", None)

#adding the odds to a dataframe
pd.DataFrame({"Odds": odds, "Change_odd %": perc_change_odds}, index=X_train2.columns).T

"""Example of coefficient interpretations:
- no_of_adults: Holding all other features constant, a 1 unit change in number of adults will increase the odds of a booking being canceled by ~1.1 times or a ~11.5% increase in odds of a booking being canceled. Likewise for children, a 1 unit increase will increase the odds of a booking being canceled by ~1.165 times or a 16.5 percent increase in odds.

- required_car_parking_space: Holding all other features constant, a 1 unit change (going from 0 to 1: or not needing a parking space to needing a parking space) will decrease the odds of a booking being canceled by ~0.2 times or a ~79.7% decrease in odds of a booking being canceled.

Interpretation for other attributes can be looked at similarly.

## Model performance evaluation

We will create a function to calculate different metrics as well as the confusion matrix for our model and subsquent models.

When looking at performance metrics, we will look at accuracy, precision, recall, and f1 score. The score that will likely be most important to the model for this particular case is f1 score. This is because f1 score maximizes both precision and recall, which means that both false negatives and false positives are reduced.

Although it is important for the hotel to avoid false negatives (not identifying a likely cancellation), it is also important to avoid false positives (mistakenly identifying a cancellation that would not have happened) in order to prevent losing business by assuming a booking will be canceled when it isn't.
"""

#defining a function to compute different metrics to check performance of our model
def model_perf_classification(
    model, predictors, target, threshold=0.5
):
  """
  model: classifier
  predictors: independent variables
  target: dependent variable
  threshold: threshold for classifying the observation as class1
  """

  #checking which probabilites are greater than threshold
  pred_temp = model.predict(predictors) > threshold
  #rounding off the above values to get classes
  pred = np.round(pred_temp)

  acc = accuracy_score(target, pred) #compute accuracy
  recall = recall_score(target, pred) #compute recall
  precision = precision_score(target, pred) #compute precision
  f1 = f1_score(target, pred) #compute F1-score

  #creating a datagrame for these metrics
  df_perf = pd.DataFrame(
      {"Accuracy": acc, "Recall": recall, "Precision": precision, "F1": f1},
      index=[0],
  )

  return df_perf

#defining a function to plot the confusion matrix of our model

def confusion_matrix_statsmodels(model, predictors, target, threshold=0.5):
  """
  model: classifier
  predictors: independent variables
  target: dependent variable
  threshold: threshold for classifying the observation as Class 1
  """
  y_pred = model.predict(predictors) > threshold
  cm = confusion_matrix(target, y_pred)
  labels = np.asarray(
      [
          ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
          for item in cm.flatten()
      ]
  ).reshape(2,2)

  plt.figure(figsize=(6,4))
  sns.heatmap(cm, annot=labels, fmt="")
  plt.ylabel("True label")
  plt.xlabel("Predicted label")

"""**Training set performance**"""

#checking confusion matrix for our model
confusion_matrix_statsmodels(lg2, X_train2, Y_train)

"""Observations:
- 15,161 or 59.71% of bookings were correctly predicted as Non-cancellations **- True Negatives (TN)**
- 5,291 or 20.84% of bookings were correctly predicted as Cancellations **- True Positives (TP)**
- 3,072 or 12.10% of bookings were incorrectly predicted as Non-cancellations **- False Negatives (FN)**
- 1,868 or 7.36% of bookings were incorrectly predicted as Cancellations **- False Positives (FP)**


"""

#checking performance metrics on our model
print("Training Performance:")
default_threshold_train = model_perf_classification(lg2, X_train2, Y_train)
default_threshold_train

"""Accuracy and precision are showing pretty good scores, however the recall and f1 scores are lower than 0.70.

**Test set performance**
"""

#first we need to drop the same columns in the test set that were dropped in the training set
X_test2 = X_test[list(X_train2.columns)]

#creating confusion matrix for test set
confusion_matrix_statsmodels(lg2, X_test2, Y_test)

"""The confusion matrix for the test set has comparable percentages for TP, TN, FP, and FN as the training set."""

print("Test Performance:")
default_threshold_test = model_perf_classification(lg2, X_test2, Y_test)
default_threshold_test

"""The test data is showing very comparable metrics as the training data. This tells us that our model is not overfitting.

**Model Performance Improvement**

We will now try to improve the model by adjusting the model threshold.  First we will check the ROC and use it to find the optimal threshold based on the area under the curve (ROC-AUC). Then, we will check the precision-recall curve to check the threshold which optimizes both precision and recall. This will help us maximize the f1 score, which takes into account both precision and recall.
"""

logit_roc_train = roc_auc_score(Y_train, lg2.predict(X_train2))
fpr, tpr, thresholds = roc_curve(Y_train, lg2.predict(X_train2))
plt.figure(figsize=(7,5))
plt.plot(fpr, tpr, label = "Logistic Regression (area = %0.2f)" % logit_roc_train)
plt.plot([0,1], [0,1], "r--")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver operating characteristic")
plt.legend(loc = "lower right")
plt.show()

"""The model is giving a good performance on the training set. We can try to improve further by finding the optimal threshold based on the ROC curve above."""

#finding optimal threshold per AUC-ROC curve (where true positive rate (tpr) is high and false positive rate (fpr) is low)
fpr, tpr, thresholds = roc_curve(Y_train, lg2.predict(X_train2))

optimal_idx = np.argmax(tpr-fpr)
optimal_threshold_auc_roc = thresholds[optimal_idx]
print(optimal_threshold_auc_roc)

"""We will now check the model performance on the training and test set using the optimal roc_auc threshold of ~0.37.

**ROC-AUC - Training Set performance**
"""

confusion_matrix_statsmodels(lg2, X_train2, Y_train, threshold = optimal_threshold_auc_roc)

"""Compared to the initial model, the percentage of true positives has gone down but the percentage of true negatives has gone up. The percentage of false positives has gone up and the percentage of false negatives has gone down."""

print("Training Set Performance:")
roc_threshold_train = model_perf_classification(lg2, X_train2, Y_train, threshold = optimal_threshold_auc_roc)
roc_threshold_train

"""Observations:
- Accuracy has decreased very slightly
- Recall has increased considerably
- Precision has decreased
- F1 has increased slightly

**ROC-AUC - Test Set Performance**
"""

logit_roc_test = roc_auc_score(Y_test, lg2.predict(X_test2))
fpr, tpr, thresholds = roc_curve(Y_test, lg2.predict(X_test2))
plt.figure(figsize=(7,5))
plt.plot(fpr, tpr, label = "Logistic Regression (area = %0.2f)" % logit_roc_test)
plt.plot([0,1], [0,1], "r--")
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver operating characteristic")
plt.legend(loc = "lower right")
plt.show()

confusion_matrix_statsmodels(lg2, X_test2, Y_test, threshold = optimal_threshold_auc_roc)

"""Observations:
The test set is giving very comparable percentages of TP, FP, TN, and FN as the training data.
"""

print("Test set performance:")
roc_threshold_test = model_perf_classification(lg2, X_test2, Y_test, threshold = optimal_threshold_auc_roc)
roc_threshold_test

"""Observations: The test set is giving very comparable performance metrics as well.

**Precision-Recall Curve**
"""

#plotting precision-recall curve

y_scores = lg2.predict(X_train2)
prec, rec, tre = precision_recall_curve(Y_train, y_scores)

def plot_prec_recall_vs_tresh(precisions, recalls, thresholds):
  plt.plot(thresholds, precisions[:-1], "b--", label="precision")
  plt.plot(thresholds, recalls[:-1], "g--", label="recall")
  plt.xlabel("Threshold")
  plt.legend(loc="upper left")
  plt.ylim([0,1])

plt.figure(figsize=(10,7))
plot_prec_recall_vs_tresh(prec, rec, tre)
plt.show()

"""We can see the precision and recall lines intersect at a threshold of around 0.42. We can try setting our model to this threshold"""

optimal_threshold_curve = 0.42

"""**Precision-Recall - Training Set Performance**"""

confusion_matrix_statsmodels(lg2, X_train2, Y_train, threshold=optimal_threshold_curve)

"""Observations: The percentage of true negatives and false positives has gone slightly up from our previous model, and the percentage of true positive and false negatives have gone slightly down compared to the previous model."""

print("Training performance:")
curve_threshold_train = model_perf_classification(lg2, X_train2, Y_train, threshold=optimal_threshold_curve)
curve_threshold_train

"""Observations:
- Accuracy has increased slightly
- Recall has decreased
- Precision has increased
- F1 has slightly decreased

**Precision-Recall - Test Set Performance**
"""

confusion_matrix_statsmodels(lg2, X_test2, Y_test, threshold=optimal_threshold_curve)

"""Observations: The percentages of TP, TN, FP, and FN are comparable to the training set."""

print("Test Set Performance:")
curve_threshold_test = model_perf_classification(lg2, X_test2, Y_test, threshold=optimal_threshold_curve)
curve_threshold_test

"""Observations: The performance metrics for the test set are very comparable to the metrics of the training set.

## Model Performance Comparison Final Model Summary

We will now compare the performance on the test and training data for all 3 thresholds.
"""

#training performance comparison

models_train_comp_df = pd.concat(
    [
        default_threshold_train.T,
        roc_threshold_train.T,
        curve_threshold_train.T,
    ],
    axis=1,
)
models_train_comp_df.columns = [
    "Logistic Regression - default threshold (0.5)",
    "Logistic Regression - 0.37 threshold",
    "Logistic Regression - 0.42 threshold"
]

print("Training performance comparison:")
models_train_comp_df

#testing performance comparison

models_test_comp_df = pd.concat(
    [
        default_threshold_test.T,
        roc_threshold_test.T,
        curve_threshold_test.T,
    ],
    axis=1
)
models_test_comp_df.columns = [
    "Logistic Regression - default threshold (0.5)",
    "Logistic Regression - 0.37 threshold",
    "Logistic Regression - 0.42 threshold",
]

print("Test set performance comparison:")
models_test_comp_df

"""Observations
- None of the models are overfitting
- The default threshold gives the highest accuracy
- The 0.37 threshold gives the highest recall
- The default threshold gives the highest precision
- The 0.37 threshold gives the highest F1

As our chosen metric is F1, we can use the model with the 0.37 threshold (ROC-AUC optimal threshold) as our final model.

## Building a Decision Tree model

We will now use a decision tree to predict which bookings will be canceled.

We will begin by redefining our X and Y and encoding categorical features.
"""

X = hotels.drop(["booking_status"], axis=1)
Y = hotels["booking_status"]

X = pd.get_dummies(X, columns=X.select_dtypes(include="category").columns.tolist(), drop_first=True)

X.head()

#splitting the data into train and test sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=1)

print("Number of rows in train data: ", X_train.shape[0])
print("Number of rows in test data: ", X_test.shape[0])
print("Percentage of classes in training set:")
print(Y_train.value_counts(normalize=True))
print("Percentage of classes in test set:")
print(Y_test.value_counts(normalize=True))

"""Observations: The test data has 30% the number of rows as the dataset, showing the data was split into a 70:30 ratio properly. Additionally, both the train and test sets have a comparable split between canceled (1) and not canceled (0) bookings.

**Building the Model**
"""

#building and fitting our model using default parameters
model = DecisionTreeClassifier(random_state=1)
model.fit(X_train, Y_train)

"""**Model Performance Evaluation**

We will create two functions to assess our model's performance. The first function will calculate the models accuracy, recall, precision, and f1 scores and the second function will print the confusion matrix.

Note that we are still looking at F1 as our primary performance metric for this specific business problem.
"""

def model_perf_classification_sklearn(model, predictors, target):
  """
  model: Classifier
  predictors: independent variables
  target: dependent variable
  """
  #predicting using independent variables
  pred = model.predict(predictors)

  acc = accuracy_score(target, pred) #accuracy score
  recall = recall_score(target, pred) #recall score
  precision = precision_score(target, pred) #precision score
  f1 = f1_score(target, pred) #F1-score

  #creating dataframe of metrics
  df_perf = pd.DataFrame(
      {"Accuracy": acc, "Recall": recall, "Precision": precision, "F1": f1},
      index=[0],
  )

  return df_perf

def confusion_matrix_sklearn(model, predictors, target):
  """
  model: classifier
  predictors: independent variables
  target: dependent variable
  """
  y_pred = model.predict(predictors)
  cm = confusion_matrix(target, y_pred)
  labels = np.asarray(
      [
          ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
          for item in cm.flatten()
      ]
  ).reshape(2,2)

  plt.figure(figsize=(6,4))
  sns.heatmap(cm, annot=labels, fmt="")
  plt.ylabel("True label")
  plt.xlabel("Predicted label")

"""**Checking Model performance on training set**"""

tree_perf_train = model_perf_classification_sklearn(model, X_train, Y_train)
tree_perf_train

confusion_matrix_sklearn(model, X_train, Y_train)

"""Observations: The model is performing very well on the training set, with ~0.99 on all 4 metrics. On the confusion matrix, there are very few False negatives and False positives (0.44% and 0.14% respectively). This could be a sign that our model is overfitting.

**Checking Model performance on Test set**
"""

tree_perf_test = model_perf_classification_sklearn(model, X_test, Y_test)
tree_perf_test

confusion_matrix_sklearn(model, X_test, Y_test)

"""Observations: The model is performing notably less well on the test set, however still generally well.

**Visualizing the Tree and Feature Importances**
"""

column_names = list(X.columns)
feature_names = column_names
print(feature_names)

#visualizing the tree

plt.figure(figsize=(20,30))
out = tree.plot_tree(
    model,
    feature_names = feature_names,
    filled=True,
    fontsize=9,
    node_ids=True,
    class_names=True,
)
for o in out:
  arrow=o.arrow_patch
  if arrow is not None:
    arrow.set_edgecolor("black")
    arrow.set_linewidth(1)
plt.show()

"""As the decision tree is so complex that it is impossible to visualize properly, we can also look at the text report of the tree."""

print(tree.export_text(model, feature_names=feature_names, show_weights=True))

#checking the importantance of the model features
importances = model.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12,12))
plt.title("Feaures Importances")
plt.barh(range(len(indices)), importances[indices], color="violet", align="center")
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel("Relative Importance")
plt.show()

"""Observations: Lead time, average price per room, and market segment type are the top 3 important features.

## Do we need to prune the tree?

To reduce the overfitting and improve our model performance, we can prune the tree in a couple of different ways: pre-pruning and post-pruning.

We will start with **Pre-pruning**, which means we will apply hyperparameters to the tree to stop it from overgrowing/overfitting in the first place.

We can do this using a tool called GridSearchCV which tests every possible hyperparameter combination from the options in which we specify, cross validates the model performance, and outputs the combination of hyperparameters which have the highest model performance.
"""

#define Decision Tree as our classifier
estimator = DecisionTreeClassifier(random_state=1)

#grid of parameters to choose from
parameters = {
    "max_depth": np.arange(1, 10, 2),
    "min_impurity_decrease": [0.00001, 0.0001, 0.001, 0.01],
    "min_samples_split": [5, 10, 20, 30, 50, 70],
    "max_leaf_nodes": [10, 25, 50, 75, 100, 150],
}

#type of scoring used to compare parameter combinations (we want to use the f1 score)
acc_scorer = make_scorer(f1_score)

#Run Grid Search
grid_obj = GridSearchCV(estimator, parameters, scoring=acc_scorer, cv=5)
grid_obj = grid_obj.fit(X_train, Y_train)

#set classifier to best combination of parameters
estimator = grid_obj.best_estimator_

estimator.fit(X_train, Y_train)

"""Observations: The best model has a maximum depth of 9, a maximum of 150 nodes on each leaf, a minimum impurity decrease of 0.0001, and a minimum samples split threshold of 5.

**Traning set performance - Pre-pruned**
"""

tree_preprune_perf_train = model_perf_classification_sklearn(estimator, X_train, Y_train)
tree_preprune_perf_train

confusion_matrix_sklearn(estimator, X_train, Y_train)

"""Observations: As expected, performance has gone down on the train set because we are intentionally reducing overfitting.

**Test set performance - Pre-pruned**
"""

tree_preprune_perf_test = model_perf_classification_sklearn(estimator, X_test, Y_test)
tree_preprune_perf_test

confusion_matrix_sklearn(estimator, X_test, Y_test)

"""Observations: Performance on the test set is very comparable to the training set, with the performance metrics slightly less on the test set. Test performance has decreased slightly compared to the initial model."""

#visualize the pre-pruned decision tree with text report
print(tree.export_text(estimator, feature_names=feature_names, show_weights=True))

#checking which are the most important features
print(pd.DataFrame(estimator.feature_importances_, columns=["Imp"], index = X_train.columns).sort_values(by="Imp", ascending=False))

"""Lead time, market segment type, and average price per room are still the 3 most important features.

Now we will try a **post-pruning** technique called **Cost-Complexity Pruning**, in which a parameter called ccp_alpha is used to indicate the threshold at which any given sub-tree needs to "reduce impurity" in the model before it is pruned. The larger the ccp_alpha given, the more nodes are pruned and the simpler the tree.

We will first create a "path", which lists the ccp_alpha score of each possible subtree within the original tree. We will print a dataframe listing each alpha score and it's corresponding level of impurity.
"""

clf = DecisionTreeClassifier(random_state=1)
path = clf.cost_complexity_pruning_path(X_train, Y_train)
ccp_alphas, impurities = abs(path.ccp_alphas), path.impurities
pd.DataFrame(path)

fig, ax = plt.subplots(figsize=(10,5))
ax.plot(ccp_alphas[:-1], impurities[:-1], marker="o", drawstyle="steps-post")
ax.set_xlabel("effective alpha")
ax.set_ylabel("total impurity of leaves")
ax.set_title("Total Impurity vs Effective Alpha for training set")
plt.show()

"""Now we can train a decision tree using the effective alphas. Note that the last value in ccp_alphas is the alpha value that pruned the whole tree (clfs[-1])."""

clfs = []
for ccp_alpha in ccp_alphas:
  clf = DecisionTreeClassifier(random_state=1, ccp_alpha=ccp_alpha)
  clf.fit(X_train, Y_train)
  clfs.append(clf)
print(
    "Number of nodes in the last tree is: {} with ccp_alpha: {}".format(
        clfs[-1].tree_.node_count, ccp_alphas[-1]
    )
)

"""Now we remove the last element in clfs and ccp_alphas, since it is the tree with only one node.

We can visualize that the number of nodes and tree depth decreases as alpha increases.
"""

clfs = clfs[:-1]
ccp_alphas = ccp_alphas[:-1]

node_counts = [clf.tree_.node_count for clf in clfs]
depth = [clf.tree_.max_depth for clf in clfs]
fig, ax = plt.subplots(2, 1, figsize=(10,7))
ax[0].plot(ccp_alphas, node_counts, marker="o", drawstyle="steps-post")
ax[0].set_xlabel("alpha")
ax[0].set_ylabel("number of nodes")
ax[0].set_title("Number of nodes vs Alpha")
ax[1].plot(ccp_alphas, depth, marker="o", drawstyle="steps-post")
ax[1].set_xlabel("alpha")
ax[1].set_ylabel("depth of tree")
ax[1].set_title("Depth vs alpha")
fig.tight_layout()

"""Now we will check the F1 score vs Alpha for the training set and test set"""

f1_train = []
for clf in clfs:
  pred_train = clf.predict(X_train)
  values_train = f1_score(Y_train, pred_train)
  f1_train.append(values_train)

f1_test = []
for clf in clfs:
  pred_test = clf.predict(X_test)
  values_test = f1_score(Y_test, pred_test)
  f1_test.append(values_test)

fig, ax = plt.subplots(figsize=(15,5))
ax.set_xlabel("alpha")
ax.set_ylabel("F1")
ax.set_title("Recall vs alpha for training and testing sets")
ax.plot(ccp_alphas, f1_train, marker="o", label="train", drawstyle="steps-post")
ax.plot(ccp_alphas, f1_test, marker="o", label="test", drawstyle="steps-post")
ax.legend()
plt.show()

#creating the model where we get highest test f1 score
index_best_model = np.argmax(f1_test)
best_model = clfs[index_best_model]
print(best_model)

"""**Traning Set Performance - Post-pruned**"""

confusion_matrix_sklearn(best_model, X_train, Y_train)

tree_post_perf_train = model_perf_classification_sklearn(best_model, X_train, Y_train)
tree_post_perf_train

"""The model performs very well on the training set after post pruning.

**Test Set Performance - Post-pruned**
"""

confusion_matrix_sklearn(best_model, X_test, Y_test)

tree_post_perf_test = model_perf_classification_sklearn(best_model, X_test, Y_test)
tree_post_perf_test

"""The model is performing very well on the test set as well as comparably to the train set afetr post-pruning.

**Visualize post-pruned tree**
"""

plt.figure(figsize=(20,50))

out = tree.plot_tree(
    best_model,
    feature_names = feature_names,
    filled=True,
    fontsize=9,
    node_ids=False,
    class_names=None,
)
for o in out:
  arrow = o.arrow_patch
  if arrow is not None:
    arrow.set_edgecolor("black")
    arrow.set_linewidth(1)

plt.show()

#text report
print(tree.export_text(best_model, feature_names = feature_names, show_weights=True))

"""**Feature Importances for post-pruned tree**"""

print(pd.DataFrame(best_model.feature_importances_, columns=["Imp"], index=X_train.columns).sort_values(by="Imp", ascending=False))

importances = best_model.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12,12))
plt.title("Feature Importances")
plt.barh(range(len(indices)), importances[indices], color="violet", align="center")
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel("Relative Importance")
plt.show()

"""Observations: The size of the post pruned decision tree is notably smaller than the original. The 3 most importance features have still not changes frmo lead_time, avg_price_per_room and market_segment_type.

## Model Performance Comparison and Conclusions
"""

#training performance comparison

models_train_comp_df = pd.concat(
    [
        tree_perf_train.T,
        tree_preprune_perf_train.T,
        tree_post_perf_train.T,
    ],
    axis=1,
)
models_train_comp_df.columns = [
    "Decision Tree sklearn",
    "Decision Tree (Pre-Pruning)",
    "Decision Tree (Post_Pruning)",
]
print("Training performance comparison:")
models_train_comp_df

#test performance comparison

models_test_comp_df = pd.concat(
    [
        tree_perf_test.T,
        tree_preprune_perf_test.T,
        tree_post_perf_test.T,
    ],
    axis=1,
)
models_test_comp_df.columns = [
    "Decision Tree sklearn",
    "Decision Tree (Pre-Pruning)",
    "Decision Tree (Post-Pruning)"
]
print("Test performance comparison:")
models_test_comp_df

"""The decision tree with post pruning is giving the highest f1 score on the test set.

## Actionable Insights and Recommendations

- What profitable policies for cancellations and refunds can the hotel adopt?
- What other recommedations would you suggest to the hotel?

**Conclusions and recommendations:**

- Average price per room, lead time, and market segment type are the most important variables in determing whether a guest will cancel an appointment.
- Since a higher lead time leads to a higher likelihood of cancellation, the hotel could implement an automatic email reminder notification/confirmation system in which the guest must confirm their appointment at around 45 days before their stay (if booked out that far), another 14 days before (if booked less than 45 days out), as well as 3-4 days beforehand. If the guest cancels at the 45 day mark, the guest may receive a full refund, but if they cancel within 3-4 days, they do not get a refund or only get a partial refund.
- Repeated guests have a lower likelihood of canceling an appointment. The hotel could offer award or incentive programs for each stay. For example: after 3 or 5 stays, the guest gets a discount on their room. Another option is a points program in which the guest earns points with each stay and can use those points to recieve perks like a free meal.
- The hotel could offer specials for bookings under the corporate market segment to give incentive for more bookings with this demographic, as they are less likely to cancel than other market segment types (with the exception of "complimentary").
- The hotel could focus on marketing their higher priced rooms, as guests that book these are a bit less likely to cancel and risk losing that money.
- The hotel is already taking advantage of the increase in business they recieve in August to October by increasing their prices during these months, as well as decreasing their prices during less busy months such as December and January. The hotel should continue with this practice.
- There is a spike in cancellations around July. The hotel could propose a slightly higher cancellation fee or more strict refund policy during this month.
"""