# -*- coding: utf-8 -*-
"""JoannaSalvucci_USL_Project_LearnerNotebook_FullCode.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PvyRoe_Pj9_9q0WysL9nMzmefQlebVCs

# Unsupervised Learning: Trade&Ahead

**Marks: 60**

### Context

The stock market has consistently proven to be a good place to invest in and save for the future. There are a lot of compelling reasons to invest in stocks. It can help in fighting inflation, create wealth, and also provides some tax benefits. Good steady returns on investments over a long period of time can also grow a lot more than seems possible. Also, thanks to the power of compound interest, the earlier one starts investing, the larger the corpus one can have for retirement. Overall, investing in stocks can help meet life's financial aspirations.

It is important to maintain a diversified portfolio when investing in stocks in order to maximise earnings under any market condition. Having a diversified portfolio tends to yield higher returns and face lower risk by tempering potential losses when the market is down. It is often easy to get lost in a sea of financial metrics to analyze while determining the worth of a stock, and doing the same for a multitude of stocks to identify the right picks for an individual can be a tedious task. By doing a cluster analysis, one can identify stocks that exhibit similar characteristics and ones which exhibit minimum correlation. This will help investors better analyze stocks across different market segments and help protect against risks that could make the portfolio vulnerable to losses.


### Objective

Trade&Ahead is a financial consultancy firm who provide their customers with personalized investment strategies. We have been hired as a Data Scientist and have been provided with data comprising stock price and some financial indicators for a few companies listed under the New York Stock Exchange. They have assigned us the tasks of analyzing the data, grouping the stocks based on the attributes provided, and sharing insights about the characteristics of each group.

### Data Dictionary

- Ticker Symbol: An abbreviation used to uniquely identify publicly traded shares of a particular stock on a particular stock market
- Company: Name of the company
- GICS Sector: The specific economic sector assigned to a company by the Global Industry Classification Standard (GICS) that best defines its business operations
- GICS Sub Industry: The specific sub-industry group assigned to a company by the Global Industry Classification Standard (GICS) that best defines its business operations
- Current Price: Current stock price in dollars
- Price Change: Percentage change in the stock price in 13 weeks
- Volatility: Standard deviation of the stock price over the past 13 weeks
- ROE: A measure of financial performance calculated by dividing net income by shareholders' equity (shareholders' equity is equal to a company's assets minus its debt)
- Cash Ratio: The ratio of a  company's total reserves of cash and cash equivalents to its total current liabilities
- Net Cash Flow: The difference between a company's cash inflows and outflows (in dollars)
- Net Income: Revenues minus expenses, interest, and taxes (in dollars)
- Earnings Per Share: Company's net profit divided by the number of common shares it has outstanding (in dollars)
- Estimated Shares Outstanding: Company's stock currently held by all its shareholders
- P/E Ratio: Ratio of the company's current stock price to the earnings per share
- P/B Ratio: Ratio of the company's stock price per share by its book value per share (book value of a company is the net difference between that company's total assets and total liabilities)

## Importing necessary libraries and data
"""

#reading and manipulating data
import numpy as np
import pandas as pd

#data visualization
import matplotlib.pyplot as plt
import seaborn as sns

#remove limit for number of displayed columns and rows
pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", 200)

#compute distances
from scipy.spatial.distance import cdist, pdist

#scale the data using z-score
from sklearn.preprocessing import StandardScaler

#perform kmeans clustering
from sklearn.cluster import KMeans

#compute and visualize silhouette scores
from sklearn.metrics import silhouette_score
from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer

#perform PCA
from sklearn.decomposition import PCA

#performa hierarchical clustering, coompute cophenetic correlation, create dendrograms
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage, cophenet

#suppress warnings
import warnings
warnings.filterwarnings("ignore")

#loading the dataset
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/pgp_module7/project/stock_data.csv')

#making a copy of the dataframe to avoid changes to the original
data = df.copy()

"""## Data Overview

- Observations
- Sanity checks
"""

#checking a random sample of the dataset
data.sample(n=10, random_state=1)

"""The dataset has been loaded properly."""

data.shape

"""The dataset has 340 rows and 15 columns (features)."""

data.info()

"""The columns "Ticker Symbol", "Security", "GICS Sector", and "GICS Sub Industry" are of object datatype, while the rest of the columns are of numerical/continuous datatype (float or integer types)."""

#checking statistical summary of the dataset
data.describe(include="all").T

"""Observations:
- The Ticker Symbol and Security columns have all unique values, as the top values only have a frequency of 1.
- There are 11 GICS Sector values, the most frequent being Industrials with 53 values.
- There are 104 GICS Sub Industry values, the most frequent being Oil & Gas Exploration & Production, with 16 values.
- The Current price ranges from 4.50 dollars to over 1000 dollars with an average of 80 dollars. The median is lower at 60 dollars.
- In the last 13 weeks, price changes have ranged from -47 dollars to 55 dollars, with an average of 4 dollars.
- The standard deviation from the stock price, or volatility, over the last 13 weeks ranges from 0.73 to 4.58, with an average of 1.52.
- The ROE ranges from 1 to 917, with a mean of 39.5. The median is lower at 15.
- The Cash Ratio ranges from 0 to 958, with an average of 70. The median is smaller at 47.
- The Net Cash Flow ranges widely from less than -11,000,000,000 dollars to more than 20,000,000,000 dollars. The mean is around 55,000,000 dollars and the median is much lower around 2,000,000 dollars.
- The Net Income ranges widely from around -23,000,000,000 dollars to around 24,000,000,000 dollars. The mean is around 1,500,000,000, and the median is lower at 700,000,000 dollars.
- Earnings Per Share ranges from -61 to 50. The average is around 2.8.
- Estimated Shares Outstanding ranges from around 27,000,000 to over 6,000,000,000. The average is around 577,000,000 and the median is lower at around 310,000,000.
- The P/E Ratio ranges from 2.9 from 528. The average is 32.6.
- The P/B Ratio ranges from -76 to 129. The average is -1.7.

##Exploratory Data Analysis
- EDA is an important part of any project involving data.
- It is important to investigate and understand the data better before building a model with it.
- A few questions have been mentioned below which will help guide our analysis.
- A thorough analysis of the data, in addition to the questions mentioned below, will be done.

**Questions**:

1. What does the distribution of stock prices look like?
2. The stocks of which economic sector have seen the maximum price increase on average?
3. How are the different variables correlated with each other?
4. Cash ratio provides a measure of a company's ability to cover its short-term obligations using only cash and cash equivalents. How does the average cash ratio vary across economic sectors?
5. P/E ratios can help determine the relative value of a company's shares as they signify the amount of money an investor is willing to invest in a single share of a company per dollar of its earnings. How does the P/E ratio vary, on average, across economic sectors?

###Univariate Analysis

Let's first define functions to produce histograms, boxplots, and barplots to do a univariate analysis on the features.
"""

#define function to plot a boxplot and a histogram

def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to show the density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a triangle will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram

#define function to create labeled barplots

def labeled_barplot(data, feature, perc=False, n=None):
    """
    Barplot with percentage at the top

    data: dataframe
    feature: dataframe column
    perc: whether to display percentages instead of count (default is False)
    n: displays the top n category levels (default is None, i.e., display all levels)
    """

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    plt.xticks(rotation=90, fontsize=15)
    ax = sns.countplot(
        data=data,
        x=feature,
        palette="Paired",
        order=data[feature].value_counts().index[:n],
    )

    for p in ax.patches:
        if perc == True:
            label = "{:.1f}%".format(
                100 * p.get_height() / total
            )  # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(
            label,
            (x, y),
            ha="center",
            va="center",
            size=12,
            xytext=(0, 5),
            textcoords="offset points",
        )  # annotate the percentage

    plt.show()  # show the plot

"""We can start with the columns with object datatype."""

data["Ticker Symbol"].nunique()

data["Security"].nunique()

"""Observations: As the Ticker Symbol is a unique identifier and the Security column is the name of the company, both of these columns have 340 unique values (as many unique values as there are rows)."""

#GICS Sector column
labeled_barplot(data, "GICS Sector", perc=True)

"""The most frequent GICS Sector is Industrials at 15%, followed by Financials and Health Care. The least frequent Sector is Telecommunications Services at 1.5%, followed by Consumer Staples and Materials."""

#GICS Sub Industry column
labeled_barplot(data, "GICS Sub Industry", perc=True)

data["GICS Sub Industry"].nunique()

"""There are 104 different Sub Industries within the dataset. The most frequent GICS Sub Industry is Oil & Gas Exploration & Production at 4.7%, followed by REITs and Industrial Conglomerates. The least frequent Sub Industry is Casinos & Gaming at 0.3%, followed by Networking Equipment and Trucking.

Now we will move into columns with continuous datatypes.
"""

#Current Price column
histogram_boxplot(data, "Current Price")

"""(Question 1) This column appears to be right skewed with a number of large outliers. The bulk of the data falls under about 175 dollars."""

#Price Change column
histogram_boxplot(data, "Price Change")

"""Price Change has a fairly normal distribution with a slight left skew. Most of the data appears to fall between -20 and 20 percent. There are outliers present on both sides of the boxplot."""

#Volatility column
histogram_boxplot(data, "Volatility")

"""Volatility appears to be right skewed with some large outliers. The majority of the data appears to fall between 0.5 and 2.0."""

#ROE column
histogram_boxplot(data, "ROE")

"""ROE appears to be very right skewed with a large amount of high outliers. Though the majority of the data falls under 75 or so, there are outliers higher than 900."""

#Cash Ratio column
histogram_boxplot(data, "Cash Ratio")

"""Cash Ratio column is also very right skewed. Although the majority of the data falls under 200, there are outliers at almost 1000."""

#Net Cash Flow column
histogram_boxplot(data, "Net Cash Flow")

"""Net Cash Flow appears to have a fairly bell-shaped distribution but with a large amount of outliers. While most of the data falls within -0.5e10 to 0.5e10, there are outliers around -1e10 and 2e10."""

#Net Income column
histogram_boxplot(data, "Net Income")

"""Net Income appears to be fairly bell-shaped, possibly slightly right skewed with a large amount of outliers. The majority of the data falls roughly between -0.5e10 to 0.75e10, however there are values at -2.5e10 and up to over 2.5e10."""

#Earnings Per Share column
histogram_boxplot(data, "Earnings Per Share")

"""Earnings Per Share appears to be fairly bell-shaped, possibly slightly left skewed. Most of the data falls between -20 and 20 dollars. There are some outliers on either side of the distribution."""

#Estimated Shares Outstanding column
histogram_boxplot(data, "Estimated Shares Outstanding")

"""Estimated Shares Outstanding appears very right skewed with large outliers. Most of the data falls under 2e9, however there are outliers as high as over 6e9. The median and mean are between 0.25e9 and 0.75e9."""

#P/E Ratio column
histogram_boxplot(data, "P/E Ratio")

"""P/E Ratio appears right skewed with most of the data falling under 100. While the mean and median fall around 0.25, there is another small spike in the data around 90. There are outliers present that exceed 500."""

#P/B Ratio column
histogram_boxplot(data, "P/B Ratio")

"""The P/B Ratio appears fairly normal with some outliers on either side. Most of the data falls between -50 and 50.

###Bivariate Analysis
"""

#defining a list of numeric columns
num_col = ["Current Price", "Price Change", "Volatility", "ROE", "Cash Ratio", "Net Cash Flow", "Net Income", "Earnings Per Share", "Estimated Shares Outstanding", "P/E Ratio", "P/B Ratio"]

#creating a heatmap to check correlation between variables
plt.figure(figsize=(15,7))
sns.heatmap(data[num_col].corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap="Spectral")
plt.show()

"""(Question 3) There appears to be weak to moderate correlations between most variables. There is a moderate positive correlation between Net Income and Estimated Shares Outstanding, as well as between Net Income and Earnings Per Share. There is also a moderate positive correlation between Earnings Per Share and Current Price.

There is a moderate negative correlation between ROE and Earnings Per Share, as well as between Volatility and Earnings Per Share, and between Volatiliity and Net Income.

Let's look at the change in stock prices for each economic sector.
"""

sns.boxplot(data, x="Price Change", y="GICS Sector")
plt.show()

data.groupby("GICS Sector")["Price Change"].mean()

"""The Health Care Sector has the highest change in average stock price over the last 13 months (Question 2), followed closely by Consumer Staples and Information Technology (which includes the largest maximum value in Price Change). Energy has the lowest value for price change over the last 13 months - a reduction of over 10 percent (and by far the widest range of values) - followed by Utilities and Industrials.

Let's also check the Cash Ratio for businesses in each economic sector.
"""

sns.boxplot(data, x="Cash Ratio", y="GICS Sector")
plt.show()

data.groupby("GICS Sector")["Cash Ratio"].mean()

"""Information Technology has the highest Cash Ratio on average of all the economic sectors (Question 4) and also the highest range of values, followed by Telecommunication Services, and Health Care. The sectors with the lowest Cash Ratio are Utilities, folowed by Industrials and Materials.

Now let's compare the P/E and P/B Ratios across sectors.
"""

sns.barplot(data, x="P/E Ratio", y="GICS Sector")

data.groupby("GICS Sector")["P/E Ratio"].mean()

"""(Question 5) The Information Technology sector has the highest average P/E Ratio, followed closely by Real Estate. Telecommunication Services has the lowest P/E Ratio, followed by Financials."""

sns.barplot(data, x="P/B Ratio", y="GICS Sector")

data.groupby("GICS Sector")["P/B Ratio"].mean()

"""Information Technology has the highest P/B Ratio, followed by Energy and Materials. Telecommunication Services has the lowest average P/B Ratio, followed by Consumer Discretionary and Consumer Staples.

Let's look at how the average Current Price varies by Sub Industry.
"""

a=data.groupby(["GICS Sub Industry"])[["Current Price"]].mean()
a.sort_values(by="Current Price", ascending=False)

"""Internet & Direct Marketing Retail, Life Sciences Tools & Services, and Biotechnology are the Sub Industries with the highest current stock price. Copper, Computer Hardware, and Technology Hardware, Store & Peripherals have the lowest.

Let's see how Current Price varies with Return on Equity (ROE).
"""

sns.scatterplot(data, x="Current Price", y="ROE")

"""There doesn't appear to be a strong correlation between these variables, as we noted when we analyzed the heatmap. Most values tend to cluster around the 0 to 200 mark for both variables.

Let's look at Net Income versus Estimated Shares Outstanding.
"""

sns.scatterplot(data, x="Net Income", y="Estimated Shares Outstanding")

"""As noted earlier, there appears to be a moderate positive correlation between these 2 variables. There appears to be a cluster of points around 0 to 1(e10/e9) for both variables, as is expected.

Let's look at Net Income and Earnings Per Share.
"""

sns.scatterplot(data, x="Earnings Per Share", y="Net Income")

"""Points tend to cluster around the center of the graph at 0 for both variables (-20 to 20 for Earnings Per Share and -1 to 1 (e10) for Net Income). There appears to be a moderately positive correlation between the two: As Net Income increases, Earnings Per Share tend to increase as well.

Let's look at Earnings Per Share versus ROE.
"""

sns.scatterplot(data, x="ROE", y="Earnings Per Share")

"""Points appear to cluster around the 0 (-20 to 20 marks for Earnings Per Share, and the 0 to 100 mark for ROE). There appears to be a slightly negative correlation between these two variables: As Earnings Per Share decrease, ROE generally tends to decrease as well.

## Data Preprocessing

- Duplicate value check
- Missing value treatment
- Outlier check
- Feature engineering (if needed)
- Any other preprocessing steps (if needed)
"""

#checking for duplicated values
data.duplicated().sum()

"""There are no duplicated values in the dataset."""

#checking for missing values
data.isna().sum()

"""There are no missing values in the dataset."""

#use boxplots to check for outliers
plt.figure(figsize=(15, 10))

for i, variable in enumerate(num_col):
    plt.subplot(3, 4, i + 1)
    plt.boxplot(df[variable], whis=1.5)
    plt.tight_layout()
    plt.title(variable)
plt.show()

"""Nearly every feature contains a notable amount of outliers. We could lose a lot of important information depending on how we treat the outliers.

We can use the boxplots to identify the features with the most notable outliers and check to see how many values exist in that feature over a certain value. This could help us determine if it would be useful to treat the outliers. From the boxplots, we can see that Current Price, Cash Ratio, ROE, P/E and P/B Ratio may have the most notable outliers (outliers furthest away from IQR, or from other clustered outliers).
"""

#checking number of outliers in Current Price column over 1000
data[data["Current Price"]>1000]

#checking number of outliers in Cash Ratio column over 800
data[data["Cash Ratio"]>800]

#checking number of outliers in the ROE column over 700
data[data["ROE"]>700]

#checking number of outliers in P/E Ratio column over 400
data[data["P/E Ratio"]>400]

#checking number of outliers in P/B Ratio column over 100
data[data["P/B Ratio"]>100]

"""The number of these more extreme outliers is not significant for any of the 5 columns mentioned above. Additionally, outliers can aid in forming well-defined clusters. For these reasons, we will not treat the outliers and risk losing important information. We can also try using methods less sensitive to outliers, such as Hierarchical clustering using cityblock linkage, to apply our clustering algorithm.

###Scaling the data
"""

#scaling the data before clustering
scaler=StandardScaler()
subset= data[num_col].copy()
subset_scaled = scaler.fit_transform(subset)

#create dataframe of scaled columns
subset_scaled_df = pd.DataFrame(subset_scaled, columns=subset.columns)

#checking the scaled data
subset_scaled_df.head()

"""The data has been scaled properly.

## K-means Clustering

First, we will apply KMeans clustering and calculate average distortion using 1-8 clusters, as well as checking the elbow plot. When we see a distinct bend or "elbow" in the plot of "Average distortion" against "k" (or number of clusters), we can note this as a possible good value for k. This is because a bend indicates there is a steeper drop off in average distortion between one k value and the next.
"""

clusters = range(1, 9)
meanDistortions = []

for k in clusters:
    model = KMeans(n_clusters=k)
    model.fit(subset_scaled_df)
    prediction = model.predict(subset_scaled_df)
    distortion = (
        sum(
            np.min(cdist(subset_scaled_df, model.cluster_centers_, "euclidean"), axis=1)
        )
        / subset_scaled_df.shape[0]
    )

    meanDistortions.append(distortion)

    print("Number of Clusters:", k, "\tAverage Distortion:", distortion)

plt.plot(clusters, meanDistortions, "bx-")
plt.xlabel("k")
plt.ylabel("Average distortion")
plt.title("Selecting k with the Elbow Method")
plt.show()

"""The approriate value for k seems to be around 4 or 5 based on the Elbow Method.

We can check now the silhouette score to further determine the best value of k. The higher the Silhouette score, the less likely each data point is to be assigned a different cluster, and likewise, has been assigned to the correct cluster. We will choose the k value with the highest Silhouette score.
"""

sil_score = []
cluster_list = list(range(2, 10))
for n_clusters in cluster_list:
    clusterer = KMeans(n_clusters=n_clusters)
    preds = clusterer.fit_predict((subset_scaled_df))
    centers = clusterer.cluster_centers_
    score = silhouette_score(subset_scaled_df, preds)
    sil_score.append(score)
    print("For n_clusters = {}, the silhouette score is {})".format(n_clusters, score))

plt.plot(cluster_list, sil_score)
plt.show()

"""The silhouette score for 4 clusters is higher than the score for 5. Let's compare the Silhouette Plots of Kmeans Clustering for 4 and 5 clusters, respectively."""

#KMeans clustering with 4 clusters
visualizer = SilhouetteVisualizer(KMeans(4, random_state=1))
visualizer.fit(subset_scaled_df)
visualizer.show()

#KMeans clustering with 5 clusters
visualizer = SilhouetteVisualizer(KMeans(5, random_state=1))
visualizer.fit(subset_scaled_df)
visualizer.show()

"""We can choose 4 for the value of k based on the silhouette coefficient values.

We can now move forward with creating our final model.
"""

kmeans = KMeans(n_clusters=4, random_state=1)
kmeans.fit(subset_scaled_df)

#add cluster labels to original and scaled dataframes
data["Kmeans_segments"] = kmeans.labels_
subset_scaled_df["Kmeans_segments"] = kmeans.labels_

"""###Cluster Profiling"""

#creating a dataframe with mean values grouped by Kmeans segment
cluster_profile = data.groupby("Kmeans_segments").mean()

#creating an additional column of counts for each segment
cluster_profile["count_in_each_segment"] = (data.groupby("Kmeans_segments")["Security"].count().values)

#display cluster profiles and highlight maximum values for each feature
cluster_profile.style.highlight_max(color="lightgreen", axis=0)

#companies in each cluster
for cl in data["Kmeans_segments"].unique():
    print("Companies included in cluster {}:".format(cl))
    print(data[data["Kmeans_segments"] == cl]["Security"].unique())
    print()

#checking which economic sectors are most frequent in each cluster
data.groupby(["Kmeans_segments", "GICS Sector"])['Security'].count()

#checking which sub industries are most frequent in each cluster
data.groupby(["Kmeans_segments", "GICS Sub Industry"])['Security'].count()

"""Cluster 0: Industrials and Financials are the most prominent GICS Sector for Cluster 0, with Industrial Conglomerates and REITs as the most prominent Sub Industries for this cluster.

Cluster 1: By a small amount, Financials is the most prominent GICS Sector for Cluster 1, with Banks as the most prominent Sub Industry.

Cluster 2: Energy is the most prominent GICS Sector for Cluster 2, with Oil & Gas Exploration & Production as the most prominent Sub Industry.

Cluster 3: Health Care is the most prominent GICS Sector for Cluster 3, with Biotechnology as the most prominent Sub Industry.
"""

#create bar plot comparing features among clusters
subset_scaled_df.groupby("Kmeans_segments").mean().plot.bar(figsize=(15, 6))

#create boxplots comparing features among clusters
plt.figure(figsize=(15, 10))
plt.suptitle("Boxplot of numerical variables for each cluster")

for i, variable in enumerate(num_col):
    plt.subplot(3, 4, i + 1)
    sns.boxplot(data=subset_scaled_df, x="Kmeans_segments", y=variable)

plt.tight_layout(pad=2.0)

"""Observations:

Note - all statements made below regarding "moderate", "low" and "high" values refer to such values *relative to other clusters*.

 - Cluster 0:
  - This cluster has low to moderate values for most of it's features.
  - This cluster includes moderate values for Price Change, Volatility, Net Cash Flow, Net Income, and Earnings Per Share.
  - None of the features for this cluster contain extreme values - tending to hover around 0 when scaled. P/B Ratio is the only feature which includes notable outliers.
  - This cluster includes the most amount of datapoints by far.
 - Cluster 1:
  - This cluster also tends to have low to moderate values with a few exceptions.
  - This cluster has notably high values for the Estimated Shares Oustanding and Net Income features.
  - This cluster also has a larger range of values for the Net Cash Flow feature, and its outlier values for this column extend quite low.
  - This cluster has the least amount of  datapoints.
 - Cluster 2:
  - This cluster has relatively moderate values for some features including Current Price, Cash Ratio, Net Cash Flow, Estimated Shares Outstanding, and P/B Ratio.
  - This cluster has notably lower values for Price Change, Net Income, and Earnings Per Share.
  - This cluster has notably high values for volatility. It also has moderately high values for ROE and P/E Ratio.
 - Cluster 3:
  - This cluster has low to moderate values for most features, but also with slightly larger ranges of values for most features, as compared to other clusters.
  - This cluster has higher values for Current Price, Cash Ratio and P/B Ratio.

## Hierarchical Clustering

There are a couple of different methods that can be used in Hierarchical clustering - the one that we will use is called Agglomerative Clustering. This means that each data point is treated as it's own cluster, and then clusters are continuously combined based on point distance. There are different types of "linkage" and different measures of distance that can be used, which will affect how points are combined into clusters.

One tool that we will use for this is called a Dendrogram, which is a tree diagram that represents the distance at which clusters combine. The Cophenetic correlation coefficient (referred to as just Cophenetic correlation) is a measure which indicates how accurately a Dendrogram preserves that actual pairwise distances between the points.

First, we will compute the cophenetic correlation for different combinations of linkage and distance metrics.
"""

scaled_df_hc = subset_scaled_df.copy()

#list of distance metrics
distance_metrics = ["euclidean", "chebyshev", "mahalanobis", "cityblock"]

#list of linkage methods
linkage_methods = ["single", "complete", "average", "weighted"]

high_cophenet_corr = 0
high_dm_lm = [0, 0]

for dm in distance_metrics:
    for lm in linkage_methods:
        Z = linkage(scaled_df_hc, metric=dm, method=lm)
        c, coph_dists = cophenet(Z, pdist(scaled_df_hc))
        print(
            "Cophenetic correlation for {} distance and {} linkage is {}.".format(
                dm.capitalize(), lm, c
            )
        )
        if high_cophenet_corr < c:
            high_cophenet_corr = c
            high_dm_lm[0] = dm
            high_dm_lm[1] = lm

#combination of distance metric and linkage method with the highest cophenetic correlation
print(
    "Highest cophenetic correlation is {}, which is obtained with {} distance and {} linkage.".format(
        high_cophenet_corr, high_dm_lm[0].capitalize(), high_dm_lm[1]
    )
)

"""We can explore linkage methods with Eucledian distance only."""

#list of linkage methods
linkage_methods = ["single", "complete", "average", "centroid", "ward", "weighted"]

high_cophenet_corr = 0
high_dm_lm = [0, 0]

for lm in linkage_methods:
    Z = linkage(scaled_df_hc, metric="euclidean", method=lm)
    c, coph_dists = cophenet(Z, pdist(scaled_df_hc))
    print("Cophenetic correlation for {} linkage is {}.".format(lm, c))
    if high_cophenet_corr < c:
        high_cophenet_corr = c
        high_dm_lm[0] = "euclidean"
        high_dm_lm[1] = lm

#linkage method with the highest cophenetic correlation
print(
    "Highest cophenetic correlation is {}, which is obtained with {} linkage.".format(
        high_cophenet_corr, high_dm_lm[1]
    )
)

"""The cophenetic correlation is highest when using centroid linkage with Euclidean distance, followed shortly by average linkage with Euclidean distance. Let's check the dendrograms for these methods.

###Dendrograms
"""

#list of linkage methods to create dendrograms for
linkage_methods = ["single", "complete", "average", "centroid", "ward", "weighted"]

#lists to save results of cophenetic correlation calculation
compare_cols = ["Linkage", "Cophenetic Coefficient"]
compare = []

#create a subplot image
fig, axs = plt.subplots(len(linkage_methods), 1, figsize=(15, 30))

#loop will enumerate through the list of linkage methods above
#plot the dendrogram and calculate the cophenetic correlation for each linkage method
for i, method in enumerate(linkage_methods):
    Z = linkage(scaled_df_hc, metric="euclidean", method=method)

    dendrogram(Z, ax=axs[i])
    axs[i].set_title(f"Dendrogram ({method.capitalize()} Linkage)")

    coph_corr, coph_dist = cophenet(Z, pdist(scaled_df_hc))
    axs[i].annotate(
        f"Cophenetic\nCorrelation\n{coph_corr:0.2f}",
        (0.80, 0.80),
        xycoords="axes fraction",
    )

    compare.append([method, coph_corr])

"""We can see that although centroid and average linkage had the highest cophenetic correlation scores, ward linkage has the most clearly distinct clusters.

We can also check the dendrograms for mahalanobis and cityblock distance with average and single linkage, as these methods had high cophenetic correlation scores as well.
"""

#list of distance metrics
distance_metrics = ["mahalanobis", "cityblock"]

#list of linkage methods
linkage_methods = ["average", "single"]

# to create a subplot image
fig, axs = plt.subplots(
    len(distance_metrics) + len(distance_metrics), 1, figsize=(10, 30)
)

i = 0
for dm in distance_metrics:
    for lm in linkage_methods:
        Z = linkage(scaled_df_hc, metric=dm, method=lm)

        dendrogram(Z, ax=axs[i])
        axs[i].set_title("Distance metric: {}\nLinkage: {}".format(dm.capitalize(), lm))

        coph_corr, coph_dist = cophenet(Z, pdist(scaled_df_hc))
        axs[i].annotate(
            f"Cophenetic\nCorrelation\n{coph_corr:0.2f}",
            (0.80, 0.80),
            xycoords="axes fraction",
        )
        i += 1

"""Most of the dendrograms shown have low to mildly distinct clusters."""

#dataframe to compare cophenetic correlations for each linkage method with Euclidean distance
df_cc = pd.DataFrame(compare, columns=compare_cols)
df_cc

"""We will move forward with Euclidean distance with average linkage. It appears that 6 would be the appropriate number of clusters from the associated dendrogram.

###Creating the model using Hierarchical Clustering
"""

#creating 6 clusters with hierarchical clustering method
HCmodel = AgglomerativeClustering(n_clusters=6, affinity="euclidean", linkage="average")
HCmodel.fit(scaled_df_hc)

#add cluster labels to original and scaled dataframes
scaled_df_hc["HC_Clusters"] = HCmodel.labels_
data["HC_Clusters"] = HCmodel.labels_

cluster_profile_hc = data.groupby("HC_Clusters").mean()
cluster_profile_hc["count in each segment"] = (data.groupby("HC_Clusters")["Security"].count().values)

#companies in each cluster
for cl in data["HC_Clusters"].unique():
    print("Companies included in cluster {}:".format(cl))
    print(data[data["HC_Clusters"] == cl]["Security"].unique())
    print()

"""When we look at the companies included in each of the clusters, 5 out of 6 include only 1 or 2 companies, and Cluster 0 includes the remaining companies. **We can conclude that the clusters do not have enough variability.** We will try using Ward linkage, as we can see from the dendrogram that is has more distinct clusters. It seems that 4 is the appropriate number of clusters from the dendrogram for Ward linkage.

###Building Final Hierarchical Clustering Model
"""

#creating 4 clusters with ward linkage
HCmodel1 = AgglomerativeClustering(n_clusters=4, affinity="euclidean", linkage="ward")
HCmodel1.fit(scaled_df_hc)

#add cluster labels to original and scaled dataframes
scaled_df_hc["HC_Clusters"] = HCmodel1.labels_
data["HC_Clusters"] = HCmodel1.labels_

"""###Cluster Profiling"""

cluster_profile_hc = data.groupby("HC_Clusters").mean()
cluster_profile_hc["count in each segment"] = (data.groupby("HC_Clusters")["Security"].count().values)

#companies in each cluster
for cl in data["HC_Clusters"].unique():
    print("Companies included in cluster {}:".format(cl))
    print(data[data["HC_Clusters"] == cl]["Security"].unique())
    print()

"""We now have much more variability in the clusters and will proceed with this as our final model."""

#checking which economic sectors are most frequent in each cluster
data.groupby(["HC_Clusters", "GICS Sector"])['Security'].count()

#checking which Sub Industries are most frequent in each cluster
data.groupby(["HC_Clusters", "GICS Sub Industry"])['Security'].count()

"""Cluster 0: Energy is the most prominent GICS Sector for Cluster 0, with Oil & Gas Exploration & Production as the most prominent Sub Industry.

Cluster 1: Industrials and Financials are the most prominent GICS Sector for Cluster 1, with Industrial Conglomerates and REITs as the most prominent Sub Industries for this cluster.

Cluster 2: Health Care is the most prominent GICS Sector for Cluster 2, with Biotehnology as the most prominent Sub Industry.

Cluster 3: By a small amount, Financials is the most prominent GICS Sector for Cluster 3, with Banks as the most prominent Sub Industry.
"""

#display cluster profiles
cluster_profile_hc.style.highlight_max(color="lightgreen", axis=0)

scaled_df_hc.groupby("HC_Clusters").mean().plot.bar(figsize=(15, 6))

plt.figure(figsize=(15, 10))
plt.suptitle("Boxplot of numerical variables for each cluster")

for i, variable in enumerate(num_col):
    plt.subplot(3, 4, i + 1)
    sns.boxplot(data=scaled_df_hc, x="HC_Clusters", y=variable)

plt.tight_layout(pad=2.0)

"""Observations:

Note - all statements made below regarding "moderate", "low" and "high" values refer to such values *relative to other clusters*.

 - Cluster 0:
  - This cluster includes moderate values for Current Price, Cash Ratio, Net Cash Flow, Estimated Shares Outstanding, and P/B Ratio.
  - This cluster includes lower values for Price Change, Net Income, and Earnings Per Share. This cluster also shows a larger range of values for these features, particularly for Price Change.
  - This cluster includes higher values for Volatility, ROE, and P/E Ratio. Volatility includes a large range of values.
 - Cluster 1:
  - This cluster has moderate values for most of the features. It has a larger amount of outliers than other clusters, however these outliers still tend to fall within the "moderate" range.
  - This makes sense, as Cluster 1 includes the majority of the data points.
  - This cluster has a larger range of values for Volatility and Price Change.
 - Cluster 2:
  - This cluster includes moderate values for ROE, Volatility, Net Cash Flow, Net Income, Earnings Per Share, and Estimated Shares Outstanding.
  - This cluster includes higher values for Current Price, Price Change, Cash Ratio, and P/B Ratio, as compared to other clusters.
  - This cluster has high outliers for Current Price, Cash Ratio, Net Cash Flow, P/E Ratio, and P/B Ratio.
 - Cluster 3:
  - This cluster has moderate values for Cash Ratio, Earnings Per Share, P/E Ratio, and P/B Ratio.
  - This cluster has lower values for Volatility and Net Cash flow.
  - This cluster has higher values for Net Income and Estimated Shares Oustanding. The difference between Estimated Shares Oustanding value range for this cluster versus others is notable.
  - This cluster has the smallest amount of datapoints.

## K-means vs Hierarchical Clustering

- The Hierarchical Clustering technique took notably less time for computing compared to K-means Clustering.
- The clusters produced for both methods both gave a similar degree of variability in terms of count, companies and Economic Sector. Variability in feature characteristics were also very similar (as detailed below). Hierarchical Clustering may have given slightly more distinct clusters when taking into account the characterics of all features together. However, this is only a slight difference.
  - If we had used average or centroid linkage (with a higher cophenetic correlation), the hierarchical clustering technique would have produced significantly less variable clusters.
- Both algorithms returned 4 as the appropriate number of clusters (for maximum variability - see note above regarding average and centroid linkage). K-means clustering created 4 clusters with the following counts: 277, 27, 25, 11. Similarly, hierarchical clustering created 4 clusters with the following counts: 273, 32, 26, 9.
 - The cluster with the majority of the datapoints for each algorithm both included low to moderate values, hovering around 0 when scaled.
 - The cluster with the smallest amount of data points for each algorithm, both include low to moderate values as well. The cluster for both algorithms also included higher values for Net Income and Estimated Shares Outstanding, in addition to lower values for Volatility.
 - Cluster 2 for K-means Clustering appears to correspond to Cluster 0 for Hierarchical clustering, as they both have moderate values overall - with low to moderate values for Price Change, Net Income, and Earnings Per Share, and higher values for Volatility, ROE, and P/E Ratio.
 - Cluster 3 for K-means Clustering appears to correspond to Cluster 2 for Hierarchical Clustering, as they both appear to have overall moderate values - with higher values for Current Price, Price Change, Cash Ratio, and P/B Ratio.
- Overall, the two algorithms produced very similar results.

###Dimensionality Reduction using PCA for Visualization

We can use Principal Component Analysis (PCA) to reduce the dimensionality of the data in order to visualize it in two dimensions. This way, we can see how well-separated the clusters are. Let's start with K-means clustering.
"""

#set the number of features to 2
pca = PCA(n_components=2)

#transform data and storing results in a dataframe
X_reduced_pca_k = pca.fit_transform(subset_scaled_df)
reduced_df_pca_k = pd.DataFrame(
    data=X_reduced_pca_k, columns=["Component 1", "Component 2"]
)

#checking the amount of variance explained
pca.explained_variance_ratio_.sum()

"""The first two principal components explain 38.6% of the variance in the data."""

sns.scatterplot(
    data=reduced_df_pca_k,
    x="Component 1",
    y="Component 2",
    hue=data["Kmeans_segments"],
    palette="rainbow",
)
plt.legend(bbox_to_anchor=(1, 1))

"""Let's do the same with Hierarchical Clustering."""

#set the number of features to 2
pca = PCA(n_components=2)

#transform data and storing results in a dataframe
X_reduced_pca_hc = pca.fit_transform(scaled_df_hc)
reduced_df_pca_hc = pd.DataFrame(
    data=X_reduced_pca_hc, columns=["Component 1", "Component 2"]
)

#checking the amount of variance explained
pca.explained_variance_ratio_.sum()

"""The first two principal components explain 38.9% of the variance in the data."""

sns.scatterplot(
    data=reduced_df_pca_hc,
    x="Component 1",
    y="Component 2",
    hue=data["HC_Clusters"],
    palette="rainbow",
)
plt.legend(bbox_to_anchor=(1, 1))

"""The PCA graphs for K-means clustering and Hierarchical Clustering appear to be similar, as expected. For simplicity, we will analyze the above graph (Hierarchical clustering) on behalf of both algorithms.
 - As we noted from our analysis, it appears the main cluster with the most data points (teal) includes moderate values hovering around 0 when scaled.
 - The cluster with the fewest data points (red) consist of mainly lower values for component 1 and low to moderate values for component 2.
 - The next cluster (yellow) consists mainly of higher values for Component 2 and low to moderate values of Component 1.
 - The final cluster (purple) consists mainly of higher values for Component 1, and moderate values for Component 2.

Although the colors are not highly separated, they do follow a gerneral pattern which makes sense in the context of clustering.

## Actionable Insights and Recommendations

The below recommendations are based on the Hierarchical Clustering method.

- Cluster 0 companies have lower values for Net Income and Earnings Per Share, but high values for Volatility and P/E Ratio. This does not seem promising, however this cluster does tend to have higher ROE values.
- Cluster 1 companies would be relatively "safe" companies to invest in, as they tend to have moderate values across all features.
- Cluster 2 companies may be less ideal and more costly to invest in, as they have higher values for Current Price, Price Change, and P/B Ratio, indicating that the stock may be overvalued. However, this also may mean that investors are expecting high growth rates for these companies going forward.
- Cluster 3 companies, though minimal, would also be decent choices to invest in, as they have moderate values overall with low values for Volatility, and higher values for Net Income. However, it has lower values for Net Cash flow and very high values for Estimated Shares Oustanding. This would indicate some risk as well.
"""