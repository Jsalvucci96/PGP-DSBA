# -*- coding: utf-8 -*-
"""MT_Project_LearnerNotebook_FullCode_JoannaSalvucci.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16cejA-fKwvPRJ1Ue6BSS5K41-kw0Hacy

## Problem Statement

### Business Context

Renewable energy sources play an increasingly important role in the global energy mix, as the effort to reduce the environmental impact of energy production increases.

Out of all the renewable energy alternatives, wind energy is one of the most developed technologies worldwide. The U.S Department of Energy has put together a guide to achieving operational efficiency using predictive maintenance practices.

Predictive maintenance uses sensor information and analysis methods to measure and predict degradation and future component capability. The idea behind predictive maintenance is that failure patterns are predictable and if component failure can be predicted accurately and the component is replaced before it fails, the costs of operation and maintenance will be much lower.

The sensors fitted across different machines involved in the process of energy generation collect data related to various environmental factors (temperature, humidity, wind speed, etc.) and additional features related to various parts of the wind turbine (gearbox, tower, blades, break, etc.).



## Objective
“ReneWind” is a company working on improving the machinery/processes involved in the production of wind energy using machine learning and has collected data of generator failure of wind turbines using sensors. They have shared a ciphered version of the data, as the data collected through sensors is confidential (the type of data collected varies with companies). Data has 40 predictors, 20000 observations in the training set and 5000 in the test set.

The objective is to build various classification models, tune them, and find the best one that will help identify failures so that the generators could be repaired before failing/breaking to reduce the overall maintenance cost.
The nature of predictions made by the classification model will translate as follows:

- True positives (TP) are failures correctly predicted by the model. These will result in repairing costs.
- False negatives (FN) are real failures where there is no detection by the model. These will result in replacement costs.
- False positives (FP) are detections where there is no failure. These will result in inspection costs.

It is given that the cost of repairing a generator is much less than the cost of replacing it, and the cost of inspection is less than the cost of repair.

“1” in the target variables should be considered as “failure” and “0” represents “No failure”.

## Data Description
- The data provided is a transformed version of original data which was collected using sensors.
- Train.csv - To be used for training and tuning of models.
- Test.csv - To be used only for testing the performance of the final best model.
- Both the datasets consist of 40 predictor variables and 1 target variable

## Importing necessary libraries
"""

# Commented out IPython magic to ensure Python compatibility.
#libraries for reading/manipulating data
import numpy as np
import pandas as pd

#libraries for data visualization
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns

#imputing missing values
from sklearn.impute import SimpleImputer

#scaling and encoding
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder

#model building
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import (
    AdaBoostClassifier,
    GradientBoostingClassifier,
    RandomForestClassifier,
    BaggingClassifier,
)
from xgboost import XGBClassifier

#tuning models and splitting data
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score

#getting performance metrics
from sklearn import metrics
from sklearn.metrics import(
    f1_score,
    accuracy_score,
    recall_score,
    precision_score,
    confusion_matrix,
    roc_auc_score,
    confusion_matrix,
)

#oversampling and undersampling data
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

#creating pipelines and personalizing
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer

#max columns to be displayed
pd.set_option("display.max_columns", None)

#suppress scientific notations for a dataframe
pd.set_option("display.float_format", lambda x: "%.3f" % x)

#suppress the warnings
import warnings

warnings.filterwarnings("ignore")

"""## Loading the dataset"""

from google.colab import drive
drive.mount('/content/drive')

train_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/pgp_module6/project/Train.csv.csv')
test_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/pgp_module6/project/Test.csv.csv')

train_df = train_data.copy()
test_df = test_data.copy()

"""## Data Overview

- Observations
- Sanity checks
"""

#checking the first and last 5 columns of the train data
train_df.head()

train_df.tail()

"""The train dataset has been loaded properly."""

#checking the shape of the train data
train_df.shape

"""The train dataset has 20,000 rows and 41 columns."""

#checking the first and last 5 rows of the test data
test_df.head()

test_df.tail()

"""The test data has been loaded properly."""

test_df.shape

"""The test data has 5000 rows and 41 columns."""

#checking data types for train data
train_df.info()

"""All of the columns in the train dataset are of float datatype except for the target (dependent variable) which is an integer datatype. There are no categorical datatypes."""

test_df.info()

"""As expected, the datatypes in the test dataset are the same as the train dataset."""

#checking for duplicated values in the train dataset
train_df.duplicated().sum()

#checking for duplicated values in the test dataset
test_df.duplicated().sum()

"""There are no duplicate values in either dataset."""

#checking for missing values in the train dataset
train_df.isna().sum()

#checking for missing values in the test dataset
test_df.isna().sum()

"""There are missing values in the V1 and V2 columns in both the train and test datasets.

## Exploratory Data Analysis (EDA)
"""

#printing the statistical summary of the train data
train_df.describe(include="all").T

"""Observations:
- Excluding the target variable, column values appear to have relatively similar ranges
  - The median value in a given column ranges from -3.533 (V21) to 2.256 (V3)
  - The mean value in a given column ranges from -3.611 (V21) to 2.485 (V3)
  - The minimum value in a given column ranges from -20.374 (V16) to -5.478 (V37)
  - The maximum value in a given column ranges from 5.671 (V14) to 23.633 (V32)
  - We wil not treat negatives values as we have been informed they are not anomalous.
- The target variable consists of only 0's and 1's (0 = No failure, 1 = Failure)
"""

#checking class distribution of the dependent variable for the train set
train_df["Target"].value_counts(1)

#checking class distribution of the dependent variable for the test set
test_df["Target"].value_counts(1)

"""The distribution of the Target columns is 94% non-failure (0) and 6% failure (1) for both the train and test set.

### Plotting histograms and boxplots for all the variables
"""

# function to plot a boxplot and a histogram along the same scale.


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    """
    Boxplot and histogram combined

    data: dataframe
    feature: dataframe column
    figsize: size of figure (default (12,7))
    kde: whether to the show density curve (default False)
    bins: number of bins for histogram (default None)
    """
    f2, (ax_box2, ax_hist2) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid= 2
        sharex=True,  # x-axis will be shared among all subplots
        gridspec_kw={"height_ratios": (0.25, 0.75)},
        figsize=figsize,
    )  # creating the 2 subplots
    sns.boxplot(
        data=data, x=feature, ax=ax_box2, showmeans=True, color="violet"
    )  # boxplot will be created and a star will indicate the mean value of the column
    sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="winter"
    ) if bins else sns.histplot(
        data=data, x=feature, kde=kde, ax=ax_hist2
    )  # For histogram
    ax_hist2.axvline(
        data[feature].mean(), color="green", linestyle="--"
    )  # Add mean to the histogram
    ax_hist2.axvline(
        data[feature].median(), color="black", linestyle="-"
    )  # Add median to the histogram

"""### Plotting all the features at one go"""

for feature in train_df.columns:
    histogram_boxplot(train_df, feature, figsize=(12, 7), kde=False, bins=None)

"""Observations:
With the exception of the target variable, all of the features have relatively normal distributions. A few of the variables appear slightly skewed: for example, V1 and V18 appear slightly right-skewed, and V8 and V10 appear slightly left-skewed. Overall, each of the 40 features have a generalized bell shape distribution.

The target variable is displaying a very imbalanced distribution with much more 0's or non-failures, as we noted earlier.

###Bivariate Analysis
Since the data has been ciphered in order to maintain confidential information, there is not much use in a Bivariate Analysis, as we do not know what each variable represents. However, we can plot a heatmap to see if there are general trends regarding feature correlation.
"""

plt.figure(figsize=(30, 20))
sns.heatmap(train_df.corr(), annot=True, vmin=-1, vmax=1, fmt=".1g", cmap="Spectral")
plt.show()

"""Observations:
We can see that there are several features that have moderate and high correlations with each other. For example:
 - V14 and V2 have a very high negative correlation
 - V7 and V15 have a very high positive correlation
 - V29 and V30 have a moderately high positive correlation
 - V33 and V25 have a moderately high negative correlation

However, since there are so many variables all with ciphered data, we are not able to analyze these correlations, and rather can just note that a few notable correlations between features do exist.

## Data Pre-processing

- As we have all continuous data types, we do not need to encode any variables.

- We also do not need to treat outliers, as we have been informed that the data has been ciphered for confidentiality and that the values presented are real values.

- We will treat missing values in the V1 and V2 columns after splitting the data so as to prevent data leakage.
"""

#defining dependent and independent variables for the train set
X = train_df.drop(["Target"], axis=1)
Y = train_df["Target"]

"""Note that we do not need to split the data into 3 parts (training, validation, and testing), as we have a separate testing dataset. We will simply split the train dataset into training and validation."""

#splitting the train dataset into training and validation set
X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.25, random_state=1, stratify=Y)

print("Shape of train data:", X_train.shape)
print("Shape of validation data:", X_val.shape)

"""There are 15000 rows in the train set, and 5000 rows in the validation set. Both sets have 40 columns. Our data has been split properly."""

print("Class distribution of train data:\n", Y_train.value_counts(1))
print("Class distribution of validation data:\n", Y_val.value_counts(1))

"""The class distributions in the train and validation sets are the same as the original data."""

#defining the dependent and independent variables for the test set
X_test = test_df.drop(["Target"], axis=1)
Y_test = test_df["Target"]

print("Shape of test data:", X_test.shape)

"""There are 5000 rows in the test set and 40 columns."""

print("Class distribution of test data:\n", Y_test.value_counts(1))

"""The test set has the same class distribution as the original data.

Now that we have split the data, we can treat the missing values. We will use SimpleImputer to impute missing values.

## Missing value imputation
"""

#confirming which columns have missing values
train_df.isna().sum()

test_df.isna().sum()

"""Only columns V1 and V2 have missing values in both the train and set datasets."""

#defining the imputer
imputer = SimpleImputer(strategy="median")

#fit and transform training data
X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)

#transform validation data
X_val = pd.DataFrame(imputer.transform(X_val), columns=X_train.columns)

#transform testing data
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_train.columns)

#confirming that no columns in the train, validation, or test sets have missing values after imputation
X_train.isna().sum()

X_val.isna().sum()

X_test.isna().sum()

"""There are no missing values in the train, validation, or test sets. We can move forward with model building.

## Model Building

### Model evaluation criterion

The nature of predictions made by the classification model will translate as follows:

- True positives (TP) are failures correctly predicted by the model.
- False negatives (FN) are real failures in a generator where there is no detection by model.
- False positives (FP) are failure detections in a generator where there is no failure.

**Which metric to optimize?**

* We need to choose the metric which will ensure that the maximum number of generator failures are predicted correctly by the model.
* We would want Recall to be maximized as greater the Recall, the higher the chances of minimizing false negatives.
* We want to minimize false negatives because if a model predicts that a machine will have no failure when there will be a failure, it will increase the maintenance cost.

**Let's define a function to output different metrics (including recall) on the train and test set and a function to show confusion matrix so that we do not have to use the same code repetitively while evaluating models.**
"""

# defining a function to compute different metrics to check performance of a classification model built using sklearn
def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # predicting using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # to compute Accuracy
    recall = recall_score(target, pred)  # to compute Recall
    precision = precision_score(target, pred)  # to compute Precision
    f1 = f1_score(target, pred)  # to compute F1-score

    # creating a dataframe of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": acc,
            "Recall": recall,
            "Precision": precision,
            "F1": f1

        },
        index=[0],
    )

    return df_perf

#function to print confusion matrix
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion_matrix with percentages

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)
    labels = np.asarray(
        [
            ["{0:0.0f}".format(item) + "\n{0:.2%}".format(item / cm.flatten().sum())]
            for item in cm.flatten()
        ]
    ).reshape(2, 2)

    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=labels, fmt="")
    plt.ylabel("True label")
    plt.xlabel("Predicted label")

"""### Defining scorer to be used for cross-validation and hyperparameter tuning

- We want to reduce false negatives and will try to maximize "Recall".
- To maximize Recall, we can use Recall as a **scorer** in cross-validation and hyperparameter tuning.
"""

# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.recall_score)

"""### Model Building with original data"""

models = []  # Empty list to store all the models

# Appending models into the list
models.append(("dtree", DecisionTreeClassifier(random_state=1)))
models.append(("Logistic Regression", LogisticRegression(random_state=1)))
models.append(("Bagging", BaggingClassifier(random_state=1)))
models.append(("Random Forest", RandomForestClassifier(random_state=1)))
models.append(("Adaboost", AdaBoostClassifier(random_state=1)))
models.append(("Gradientboost", GradientBoostingClassifier(random_state=1)))
models.append(("XGBoost", XGBClassifier(random_state=1, eval_metric="logloss")))

results1 = []  # Empty list to store all model's CV scores
names = []  # Empty list to store name of the models


# loop through all models to get the mean cross validated score
print("\n" "Cross-Validation performance on training dataset:" "\n")

for name, model in models:
    kfold = StratifiedKFold(
        n_splits=5, shuffle=True, random_state=1
    )  # Setting number of splits equal to 5
    cv_result = cross_val_score(
        estimator=model, X=X_train, y=Y_train, scoring=scorer, cv=kfold
    )
    results1.append(cv_result)
    names.append(name)
    print("{}: {}".format(name, cv_result.mean()))

print("\n" "Validation Performance:" "\n")

for name, model in models:
    model.fit(X_train, Y_train)
    scores = recall_score(Y_val, model.predict(X_val))
    print("{}: {}".format(name, scores))

"""Now that we have the scores for all of the models defined above, we can plot boxplots for each model's CV score."""

#plot boxplots for CV scores of all the models built with original data
fig = plt.figure()

fig.suptitle("Algorithm Comparison")
ax = fig.add_subplot(111)

plt.boxplot(results1)
ax.set_xticklabels(names)
plt.xticks(rotation=90)

plt.show()

"""It appears that out of the original data, the XGBoost model performs the best on CV recall (and validation recall), followed by Random Forest and Gradient Boost. Logistic Regression performs the worst on recall.

### Model Building with Oversampled data
"""

print("Before UpSampling, counts of label '1': {}".format(sum(Y_train == 1)))
print("Before UpSampling, counts of label '0': {} \n".format(sum(Y_train == 0)))

#we will use Synthetic Minority Over Sampling Technique (SMOTE)
sm = SMOTE(
    sampling_strategy=1, k_neighbors=5, random_state=1
)
X_train_over, Y_train_over = sm.fit_resample(X_train, Y_train)


print("After UpSampling, counts of label '1': {}".format(sum(Y_train_over == 1)))
print("After UpSampling, counts of label '0': {} \n".format(sum(Y_train_over == 0)))


print("After UpSampling, the shape of X_train: {}".format(X_train_over.shape))
print("After UpSampling, the shape of Y_train: {} \n".format(Y_train_over.shape))

"""The data has been properly oversampled, as the counts of labels "0" and "1" on the new datasets are equal."""

models = []  # Empty list to store all the models

# Appending models into the list
models.append(("dtree", DecisionTreeClassifier(random_state=1)))
models.append(("Logistic Regression", LogisticRegression(random_state=1)))
models.append(("Bagging", BaggingClassifier(random_state=1)))
models.append(("Random Forest", RandomForestClassifier(random_state=1)))
models.append(("Adaboost", AdaBoostClassifier(random_state=1)))
models.append(("Gradientboost", GradientBoostingClassifier(random_state=1)))
models.append(("XGBoost", XGBClassifier(random_state=1, eval_metric="logloss")))

results2 = []  # Empty list to store all model's CV scores
names = []  # Empty list to store name of the models


# loop through all models to get the mean cross validated score
print("\n" "Cross-Validation performance on training dataset:" "\n")

for name, model in models:
    kfold = StratifiedKFold(
        n_splits=5, shuffle=True, random_state=1
    )  # Setting number of splits equal to 5
    cv_result = cross_val_score(
        estimator=model, X=X_train_over, y=Y_train_over, scoring=scorer, cv=kfold
    )
    results2.append(cv_result)
    names.append(name)
    print("{}: {}".format(name, cv_result.mean()))

print("\n" "Validation Performance:" "\n")

for name, model in models:
    model.fit(X_train_over, Y_train_over)
    scores = recall_score(Y_val, model.predict(X_val))
    print("{}: {}".format(name, scores))

#plotting boxplots for CV scores of all the models built with oversampled data
fig = plt.figure()

fig.suptitle("Algorithm Comparison")
ax = fig.add_subplot(111)

plt.boxplot(results2)
ax.set_xticklabels(names)
plt.xticks(rotation=90)

plt.show()

"""Out of the models built on oversampled data, the XGBoost model again has the highest CV recall score, and the second highest validation score (the GradientBoost model has the highest validation recall score). The Random Forest model closely follows XGBoost in CV recall score. The Logistic Regression model gave the lowest CV recall score, while the dtree model gave the lowest validation recall score (although, dtree gave high CV recall).

### Model Building with Undersampled data
"""

# Random undersampler for under sampling the data
rus = RandomUnderSampler(random_state=1, sampling_strategy=1)
X_train_un, Y_train_un = rus.fit_resample(X_train, Y_train)

print("Before Under Sampling, counts of label '1': {}".format(sum(Y_train == 1)))
print("Before Under Sampling, counts of label '0': {} \n".format(sum(Y_train == 0)))

print("After Under Sampling, counts of label '1': {}".format(sum(Y_train_un == 1)))
print("After Under Sampling, counts of label '0': {} \n".format(sum(Y_train_un == 0)))

print("After Under Sampling, the shape of X_train: {}".format(X_train_un.shape))
print("After Under Sampling, the shape of Y_train: {} \n".format(Y_train_un.shape))

"""The data has been properly undersampled, as the counts of "0" and "1" in the new dataset are equal."""

models = []  # Empty list to store all the models

# Appending models into the list
models.append(("dtree", DecisionTreeClassifier(random_state=1)))
models.append(("Logistic Regression", LogisticRegression(random_state=1)))
models.append(("Bagging", BaggingClassifier(random_state=1)))
models.append(("Random Forest", RandomForestClassifier(random_state=1)))
models.append(("Adaboost", AdaBoostClassifier(random_state=1)))
models.append(("Gradientboost", GradientBoostingClassifier(random_state=1)))
models.append(("XGBoost", XGBClassifier(random_state=1, eval_metric="logloss")))

results3 = []  # Empty list to store all model's CV scores
names = []  # Empty list to store name of the models


# loop through all models to get the mean cross validated score
print("\n" "Cross-Validation performance on training dataset:" "\n")

for name, model in models:
    kfold = StratifiedKFold(
        n_splits=5, shuffle=True, random_state=1
    )  # Setting number of splits equal to 5
    cv_result = cross_val_score(
        estimator=model, X=X_train_un, y=Y_train_un, scoring=scorer, cv=kfold
    )
    results3.append(cv_result)
    names.append(name)
    print("{}: {}".format(name, cv_result.mean()))

print("\n" "Validation Performance:" "\n")

for name, model in models:
    model.fit(X_train_un, Y_train_un)
    scores = recall_score(Y_val, model.predict(X_val))
    print("{}: {}".format(name, scores))

#plotting boxplots for CV scores of all the models built with undersampled data
fig = plt.figure()

fig.suptitle("Algorithm Comparison")
ax = fig.add_subplot(111)

plt.boxplot(results3)
ax.set_xticklabels(names)
plt.xticks(rotation=90)

plt.show()

"""Of the models built on undersampled data, XGBoost has the highest CV and validation recall score followed closely by Gradientboost and Random Forest. The dtree model gave the lowest CV and validation recall scores.

###Choosing best performing models to tune

Overall, it appears that the models built on oversampled data and undersampled data performed similarly on the validation set in terms of recall, with undersampling performing slightly better. Models built on oversampled data, however, gave higher cross validation recall scores. Models built on both oversampled and undersampled data performed better on recall than models built on the original data.

We will tune the following 4 models using hyperparameter tuning with RandomSearchCV:
- XGBoost on oversampled data
  - This model has the highest CV recall
   score of all the models while still performing well on the validation set.
- XGBoost on undersampled data
  - This model has the highest validation recall score of all the models, as well as the highest CV recall score of all the models built on undersampled data. The CV recall score and validation recall score are very similar, showing consistency.
- Random Forest on undersampled data
  - This model has the second highest validation recall score, and the second highest CV recall score of the models built on undersampled data. The CV and validation recall scores are very similar, showing consistency.
- Gradient Boost on undersampled data
  - This model performs similarly to the Random Forest model on undersampled data: with high CV and validation recall scores.

## HyperparameterTuning

### Tuning XGBoost model with oversampled data
"""

# defining model
Model = XGBClassifier(random_state=1, eval_metric="logloss")

# Parameter grid to pass in RandomSearchCV
param_grid = {'n_estimators': [150, 200, 250],
              'scale_pos_weight': [5, 10],
              'learning_rate' : [0.1, 0.2],
              'gamma': [0, 3, 5],
              'subsample': [0.8, 0.9]}

#Calling RandomizedSearchCV
randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=50, n_jobs = -1, scoring=scorer, cv=5, random_state=1)

#Fitting parameters in RandomizedSearchCV
randomized_cv.fit(X_train_over, Y_train_over)

print("Best parameters are {} with CV score={}:" .format(randomized_cv.best_params_,randomized_cv.best_score_))

"""Now that RandomizedSearchCV has identified the best hyperparameters for our XGBoost model, we can create a pipeline with the best parameters."""

#building model with best parameters
xgboost_over_tuned = XGBClassifier(
    random_state=1,
    eval_metric="logloss",
    subsample= 0.9,
    n_estimators=200,
    scale_pos_weight=10,
    learning_rate=0.2,
    gamma=0)

#fitting the model on the train data
xgboost_over_tuned.fit(X_train_over, Y_train_over)

#calculating different metrics on train set
xgboost_over_tuned_train = model_performance_classification_sklearn(xgboost_over_tuned, X_train_over, Y_train_over)
xgboost_over_tuned_train

#calculating different metrics on validation set
xgboost_over_tuned_val = model_performance_classification_sklearn(xgboost_over_tuned, X_val, Y_val)
xgboost_over_tuned_val

#creating confusion matrix on validation set
confusion_matrix_sklearn(xgboost_over_tuned, X_val, Y_val)

"""XGBoost Classifier on oversampled data is performing at 100% on all metrics for the train set. For the validation set, the model is performing very high on accuracy, and high on recall, precision and F1.

### Tuning XGBoost with undersampled data
"""

# defining model
Model = XGBClassifier(random_state=1, eval_metric="logloss")

# Parameter grid to pass in RandomSearchCV
param_grid = {'n_estimators': [150, 200, 250],
              'scale_pos_weight': [5, 10],
              'learning_rate' : [0.1, 0.2],
              'gamma': [0, 3, 5],
              'subsample': [0.8, 0.9]}

#Calling RandomizedSearchCV
randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=50, n_jobs = -1, scoring=scorer, cv=5, random_state=1)

#Fitting parameters in RandomizedSearchCV
randomized_cv.fit(X_train_un, Y_train_un)

print("Best parameters are {} with CV score={}:" .format(randomized_cv.best_params_,randomized_cv.best_score_))

#building model with best parameters
xgboost_under_tuned = XGBClassifier(
    random_state=1,
    eval_metric="logloss",
    subsample=0.8,
    scale_pos_weight=10,
    n_estimators=200,
    learning_rate=0.2,
    gamma=3
)

#fitting the model on the undersampled train data
xgboost_under_tuned.fit(X_train_un, Y_train_un)

#calculating metrics on the undersampled train set
xgboost_under_tuned_train = model_performance_classification_sklearn(xgboost_under_tuned, X_train_un, Y_train_un)
xgboost_under_tuned_train

#calculating metrics on the validation set
xgboost_under_tuned_val = model_performance_classification_sklearn(xgboost_under_tuned, X_val, Y_val)
xgboost_under_tuned_val

#creating the confusion matrix on the validation set
confusion_matrix_sklearn(xgboost_under_tuned, X_val, Y_val)

"""The XGBoost Classifier model on undersampled data is performing well on the train set. On the validation set, recall and accuracy are high but precision and F1 are quite low. This indicates that while the model is good at avoiding false negatives, it is not as good at avoiding false positives. This is less important, as we have chosen recall as our scoring metric in order to reduce false negatives.

For ReneWind, this means that the model will be able to accurately identify wind turbines as defective or in need of repair (thereby reducing the number of replacements needed), but conversely will also often identify wind turbines as in need of inspection when it is not actually needed.

### Tuning RandomForest model with undersampled data
"""

# defining model
Model = RandomForestClassifier(random_state=1)

# Parameter grid to pass in RandomSearchCV
param_grid = {'n_estimators': [200, 250, 300],
              'min_samples_leaf': np.arange(1, 4),
              'max_features' : [np.arange(0.3, 0.6, 0.1), 'sqrt'],
              'max_samples': np.arange(0.4, 0.7, 0.1)}

#Calling RandomizedSearchCV
randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=50, n_jobs = -1, scoring=scorer, cv=5, random_state=1)

#Fitting parameters in RandomizedSearchCV
randomized_cv.fit(X_train_un, Y_train_un)

print("Best parameters are {} with CV score={}:" .format(randomized_cv.best_params_,randomized_cv.best_score_))

#building model with best parameters
rf_un_tuned = RandomForestClassifier(
    random_state=1,
    n_estimators=300,
    min_samples_leaf=2,
    max_samples=0.5,
    max_features= 'sqrt'
)

#fitting the model on the train data
rf_un_tuned.fit(X_train_un, Y_train_un)

#calculating different metrics on train set
rf_un_tuned_train = model_performance_classification_sklearn(rf_un_tuned, X_train_un, Y_train_un)
rf_un_tuned_train

#calculating new metrics on the validation set
rf_un_tuned_val = model_performance_classification_sklearn(rf_un_tuned, X_val, Y_val)
rf_un_tuned_val

#creating confusion matrix on validation set
confusion_matrix_sklearn(rf_un_tuned, X_val, Y_val)

"""The Random Forest Classifier on undersampled data is performing very well on the train set. Performance on the validation set for accuracy and recall is good, but with poor precision and F1 scores. Like the XGBoost model on oversampled data, this model is effective at avoiding false negatives, but less effective at avoiding false positives.

### Tuning GradientBoost with undersampled data
"""

# defining model
Model = GradientBoostingClassifier(random_state=1)

# Parameter grid to pass in RandomSearchCV
param_grid = {'n_estimators': np.arange(100, 150, 25),
              'learning_rate': [0.2, 0.05, 1],
              'subsample' : [0.5, 0.7],
              'max_features': [0.5, 0.7]}

#Calling RandomizedSearchCV
randomized_cv = RandomizedSearchCV(estimator=Model, param_distributions=param_grid, n_iter=10, n_jobs = -1, scoring=scorer, cv=5, random_state=1)

#Fitting parameters in RandomizedSearchCV
randomized_cv.fit(X_train_un, Y_train_un)

print("Best parameters are {} with CV score={}:" .format(randomized_cv.best_params_,randomized_cv.best_score_))

#building model with best parameters
gbc_under_tuned = GradientBoostingClassifier(
    random_state=1,
    subsample=0.5,
    n_estimators=100,
    max_features=0.7,
    learning_rate=0.2,
)

#fitting the model on the undersampled train set
gbc_under_tuned.fit(X_train_un, Y_train_un)

#calculating different metrics on the undersampled train data
gbc_under_tuned_train = model_performance_classification_sklearn(gbc_under_tuned, X_train_un, Y_train_un)
gbc_under_tuned_train

#calculating metrics on the validation data
gbc_under_tuned_val = model_performance_classification_sklearn(gbc_under_tuned, X_val, Y_val)
gbc_under_tuned_val

#creating the confusion matrix on the validation data
confusion_matrix_sklearn(gbc_under_tuned, X_val, Y_val)

"""Similar to the Random Forest model, the Gradient Boosting Classifier on undersampled data is performing well on the train set. Performance on the validation set for recall and accuracy is good,  while precision and F1 score are quite low.

## Model performance comparison and choosing the final model
"""

#train set performance comparison

models_train_comp_df = pd.concat(
    [
        xgboost_over_tuned_train.T,
        rf_un_tuned_train.T,
        xgboost_under_tuned_train.T,
        gbc_under_tuned_train.T,
    ],
    axis=1,
)
models_train_comp_df.columns = [
    "XGBoost Tuned - Oversampled",
    "RandomForest Tuned - Undersampled",
    "Xgboost Tuned - Undersampled",
    "Gradientboost Tuned - Undersampled",
]
print("Training performance comparison:")
models_train_comp_df

#validation set performance comparison

models_val_comp_df = pd.concat(
    [
        xgboost_over_tuned_val.T,
        rf_un_tuned_val.T,
        xgboost_under_tuned_val.T,
        gbc_under_tuned_val.T,
    ],
    axis=1,
)
models_val_comp_df.columns = [
    "XGBoost Tuned - Oversampled",
    "RandomForest Tuned - Undersampled",
    "Xgboost Tuned - Undersampled",
    "Gradientboost Tuned - Undersampled",
]
print("Validation performance comparison:")
models_val_comp_df

"""While the XGBoost Classifier on oversampled data is the highest overall performing model, the  XGBoost Classifier on undersampled data gives the highest recall score on the validation set. As we are prioritizing recall, we will proceed with this as our final model.

*Note: See discussion of recall, precision and F1 score in regards to the business problem under the "Business Insights and Conclusions" section.

We can now check our final model's performance on the test set.

### Test set final performance
"""

#checking final model performance on test set
xgboost_under_tuned_test = model_performance_classification_sklearn(xgboost_under_tuned, X_test, Y_test)
print("Test performance:")
xgboost_under_tuned_test

"""The model performs very similarly on the test set as it does on the validation set. We are getting a good recall score of 0.89 on the unseen test set.

Now we can check the most important features of the final model.
"""

feature_names = X_train.columns
importances = xgboost_under_tuned.feature_importances_
indices = np.argsort(importances)

plt.figure(figsize=(12, 12))
plt.title("Feature Importances")
plt.barh(range(len(indices)), importances[indices], color="violet", align="center")
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel("Relative Importance")
plt.show()

"""V36 is by far the most important feature for our final model, followed by V26, V10, and V39.

The least important features for this model are V12, followed by V29, V24, and V22.

## Pipelines to build the final model
"""

#creating pipeline with best parameters
Pipeline_model = Pipeline(
    steps=[
    ("impute", SimpleImputer(strategy="median")),
    ("XGBoost Classifier",
     XGBClassifier(
        random_state=1,
        eval_metric="logloss",
        subsample=0.8,
        n_estimators=200,
        scale_pos_weight=10,
        learning_rate=0.2,
        gamma=3))])

"""Note: We do not need to use column transform because there is only continuous data in our dataframe."""

#define dependent and independent variables
X1 = train_df.drop(["Target"], axis=1)
Y1 = train_df["Target"]

X_test1 = test_df.drop(["Target"], axis=1)
Y_test1 = test_df["Target"]

#missing value treatment before undersampling
imputer1 = SimpleImputer(strategy="median")
X1 = imputer1.fit_transform(X1)

#define undersampling technique
rus = RandomUnderSampler(random_state=1, sampling_strategy=1)
X_under1, Y_under1 = rus.fit_resample(X1, Y1)

#fit the model on the train data
Pipeline_model.fit(X_under1, Y_under1)

Pipeline_model_test = model_performance_classification_sklearn(Pipeline_model, X_test1, Y_test1)
Pipeline_model_test

"""After building the model with the entire train set and a pipeline, we get a high recall score of 0.88, as well as a high accuracy score. Similar to before, we are getting lower scores on precision and F1, indicating that the model is efficient at avoiding false negatives, but less efficient at avoiding false positives.

# Business Insights and Conclusions

**Conclusions:**
- The model that best reduces false negatives on the validation set was the XGBoosting Classifier on undersampled data.
  - On the test set, this model performed nearly equally as well, with a recall score of 0.88.
- This model performed very poorly on F1 and precision (resulted in a larger number of false positives), which also costs the company money due to unnecessary inspections. We chose recall as our metric because the cost of an avoidable replacement is much higher than the cost of an unecessary inspection. However, the XGboost Classifier on oversampled data could have been another useful model, as it had a similar (although slightly lower) recall score on the validation set, but a much higher F1 and precision score. This would be up to the business to decide how much risk they want to take on unnecessary replacements in exchange for lower inspection costs.
- For this model, the most important feature was V36 by far, followed by V26, V10, and V39. The least important features for this model were V12, V29, V24, and V22.
  - Since these features include ciphered data, we are not able to give further feedback or business conclusions.
- Undersampled and Oversampled data performed much better on the validation set and in cross validation than the original data.


**Business Insights:**
- It is important to indentify wind turbines that are likely to fail in order to inspect and, if needed, repair these machines before they malfunction, reducing the cost of maintenance (as inspections and repairs are much less expensive than replacements).
- However, effectively predicting which machines are defective may involve an increased risk of wind turbines being identified as defective when in fact they are not (false positive). This means those turbines will be inspected when they don't need to be, which also increases costs. However, inspections are less expensive than repairs or replacements.
- While the mchosen odel prioritized reducing the likelihood of predicting that a machine was functioning when it in fact was defective, another option could have been to choose the model with a similar (but slightly lower) ability to predict defective machines, but with a much higher ability to predict which machines were in fact functioning properly (thereby reducing the number of inspections). It is possible this may overall reduce costs for the company over time, though more research would need to be done.

***
"""